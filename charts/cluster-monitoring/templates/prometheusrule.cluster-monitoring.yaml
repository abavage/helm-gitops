apiVersion: v1
items:
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    labels:
      prometheus: k8s
      role: alert-rules
    name: alertmanager-main-rules
    namespace: cluster-monitoring
  spec:
    groups:
    - name: alertmanager.rules
      rules:
      - alert: AlertmanagerFailedReload
        annotations: 
          description: Configuration has failed to load for {{ "{{" }} $labels.namespace {{ "}}" }} / {{ "{{" }} $labels.pod {{ "}}" }}.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedReload.md
          summary: Reloading an Alertmanager configuration has failed.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max_over_time(alertmanager_config_last_reload_successful{job=~"alertmanager-main|alertmanager-user-workload"}[5m]) == 0
        for: 10m
        labels:
          severity: critical
      - alert: AlertmanagerMembersInconsistent
        annotations:
          description: Alertmanager {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.podi {{ "}}" }} has only found {{ "{{" }} $value {{ "}}" }} members of the {{ "{{" }} $labels.job {{ "}}" }} cluster.
          summary: A member of an Alertmanager cluster has not found all other cluster
            members.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            max_over_time(alertmanager_cluster_members{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
          < on (namespace,service) group_left
            count by (namespace,service) (max_over_time(alertmanager_cluster_members{job=~"alertmanager-main|alertmanager-user-workload"}[5m]))
        for: 15m
        labels:
          severity: warning
      - alert: AlertmanagerFailedToSendAlerts
        annotations:
          description: Alertmanager {{ "{{" }} $labels.namespace }}/{{ "{{" }} $labels.pod}} failed
            to send {{ "{{" }} $value | humanizePercentage }} of notifications to {{ "{{" }}$labels.integration}}.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedToSendAlerts.md
          summary: An Alertmanager instance failed to send notifications.
        expr: |
          (
            rate(alertmanager_notifications_failed_total{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
          /
            ignoring (reason) group_left rate(alertmanager_notifications_total{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
          )
          > 0.01
        for: 5m
        labels:
          severity: warning
      - alert: AlertmanagerClusterFailedToSendAlerts
        annotations:
          description: The minimum notification failure rate to {{ "{{" }} $labels.integration}} sent from any instance in the {{ "{{" }}$labels.job}} cluster is {{ "{{" }} $value | humanizePercentage }}.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerClusterFailedToSendAlerts.md
          summary: All Alertmanager instances in a cluster failed to send notifications
            to a critical integration.
        expr: |
          min by (namespace,service, integration) (
            rate(alertmanager_notifications_failed_total{job=~"alertmanager-main|alertmanager-user-workload", integration=~`.*`}[5m])
          /
            ignoring (reason) group_left rate(alertmanager_notifications_total{job=~"alertmanager-main|alertmanager-user-workload", integration=~`.*`}[5m])
          )
          > 0.01
        for: 5m
        labels:
          severity: warning
      - alert: AlertmanagerConfigInconsistent
        annotations:
          description: Alertmanager instances within the {{ "{{" }}$labels.job{{ "}}" }} cluster have
            different configurations.
          summary: Alertmanager instances within the same cluster have different configurations.
        expr: |
          count by (namespace,service) (
            count_values by (namespace,service) ("config_hash", alertmanager_config_hash{job=~"alertmanager-main|alertmanager-user-workload"})
          )
          != 1
        for: 20m
        labels:
          severity: warning
      - alert: AlertmanagerClusterDown
        annotations:
          description: '{{ "{{" }} $value | humanizePercentage {{ "}}" }} of Alertmanager instances
            within the {{ "{{" }}$labels.job{{ "}}" }} cluster have been up for less than half of
            the last 5m.'
          summary: Half or more of the Alertmanager instances within the same cluster
            are down.
        expr: |
          (
            count by (namespace,service) (
              avg_over_time(up{job=~"alertmanager-main|alertmanager-user-workload"}[5m]) < 0.5
            )
          /
            count by (namespace,service) (
              up{job=~"alertmanager-main|alertmanager-user-workload"}
            )
          )
          >= 0.5
        for: 5m
        labels:
          severity: warning
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    labels:
      app.kubernetes.io/component: operator
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: cluster-monitoring-operator
      app.kubernetes.io/part-of: openshift-monitoring
      prometheus: k8s
      role: alert-rules
    name: cluster-monitoring-operator-prometheus-rules
    namespace: cluster-monitoring
  spec:
    groups:
    - name: openshift-general.rules
      rules:
      - alert: TargetDown
        annotations:
          description: '{{ "{{" }} printf "%.4g" $value {{ "}}" }}% of the {{ "{{" }} $labels.job {{ "}}" }}/{{ "{{" }} $labels.service
            {{ "}}" }} targets in {{ "{{" }} $labels.namespace {{ "}}" }} namespace have been unreachable
            for more than 15 minutes. This may be a symptom of network connectivity
            issues, down nodes, or failures within these components. Assess the health
            of the infrastructure and nodes running these targets and then contact
            support.'
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/TargetDown.md
          summary: Some targets were not reachable from the monitoring server for
            an extended period of time.
        expr: |
          100 * ((
            1 - sum   by (job, namespace, service) (up and on(namespace, pod) kube_pod_info) /
                count by (job, namespace, service) (up and on(namespace, pod) kube_pod_info)
          ) or (
            count by (job, namespace, service) (up == 0) /
            count by (job, namespace, service) (up)
          )) > 10
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusPossibleNarrowSelectors
        annotations:
          description: Queries or/and relabel configs on Prometheus/Thanos {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }}
            could be too restrictive.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusPossibleNarrowSelectors.md
          summary: |
            Some queries or/and relabel configs with selectors on the values of the "le" label of classic histograms or/and the "quantile" label of summaries
            may not take into account that values could also be floats, they may need to be adjusted.
        expr: increase(prometheus_narrow_selectors_count{job=~"prometheus-k8s|prometheus-user-workload|thanos-querier|thanos-ruler"}[5m])
          > 0
        for: 15m
        labels:
          severity: warning
    - name: openshift-kubernetes.rules
      rules:
      - expr: sum(rate(container_cpu_usage_seconds_total{container="",pod!=""}[5m]))
          BY (pod, namespace)
        record: pod:container_cpu_usage:sum
      - expr: sum(container_fs_usage_bytes{pod!=""}) BY (pod, namespace)
        record: pod:container_fs_usage_bytes:sum
      - expr: sum(container_memory_usage_bytes{container!=""}) BY (namespace)
        record: namespace:container_memory_usage_bytes:sum
      - expr: sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[5m]))
          BY (namespace)
        record: namespace:container_cpu_usage:sum
      - expr: sum(container_memory_usage_bytes{container="",pod!=""}) BY (cluster)
          / sum(machine_memory_bytes) BY (cluster)
        record: cluster:memory_usage:ratio
      - expr: sum(container_spec_cpu_shares{container="",pod!=""}) / 1000 / sum(machine_cpu_cores)
        record: cluster:container_spec_cpu_shares:ratio
      - expr: sum(rate(container_cpu_usage_seconds_total{container="",pod!=""}[5m]))
          / sum(machine_cpu_cores)
        record: cluster:container_cpu_usage:ratio
      - expr: |
          sum by(namespace,pod, interface) (irate(container_network_receive_bytes_total{pod!=""}[5m]))
          +
          on(namespace,pod, interface) group_left(network_name) topk by(namespace,pod, interface) (1, pod_network_name_info)
        record: pod_interface_network:container_network_receive_bytes:irate5m
      - expr: |
          sum by(namespace,pod, interface) (irate(container_network_transmit_bytes_total{pod!=""}[5m]))
          +
          on(namespace,pod, interface) group_left(network_name) topk by(namespace,pod, interface) (1, pod_network_name_info)
        record: pod_interface_network:container_network_transmit_bytes_total:irate5m
      - expr: max without(endpoint, instance, job, pod, service) (kube_node_labels
          and on(node) kube_node_role{role="master"})
        labels:
          label_node_role_kubernetes_io: master
          label_node_role_kubernetes_io_master: "true"
        record: cluster:master_nodes
      - expr: max without(endpoint, instance, job, pod, service) (kube_node_labels
          and on(node) kube_node_role{role="infra"})
        labels:
          label_node_role_kubernetes_io_infra: "true"
        record: cluster:infra_nodes
      - expr: max without(endpoint, instance, job, pod, service) (cluster:master_nodes
          and on(node) cluster:infra_nodes)
        labels:
          label_node_role_kubernetes_io_infra: "true"
          label_node_role_kubernetes_io_master: "true"
        record: cluster:master_infra_nodes
      - expr: cluster:master_infra_nodes or on (node) cluster:master_nodes or on (node)
          cluster:infra_nodes or on (node) max without(endpoint, instance, job, pod,
          service) (kube_node_labels)
        record: cluster:nodes_roles
      - expr: kube_node_labels and on(node) (sum(label_replace(node_cpu_info, "node",
          "$1", "instance", "(.*)")) by (node, package, core) == 2)
        labels:
          label_node_hyperthread_enabled: "true"
        record: cluster:hyperthread_enabled_nodes
      - expr: count(sum(virt_platform) by (instance, type, system_manufacturer, system_product_name,
          baseboard_manufacturer, baseboard_product_name)) by (type, system_manufacturer,
          system_product_name, baseboard_manufacturer, baseboard_product_name)
        record: cluster:virt_platform_nodes:sum
      - expr: |
          sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id) (
            (
              cluster:master_nodes
              * on(node) group_left() max by(node)
              (
                kube_node_status_capacity{resource="cpu",unit="core"}
              )
            )
            or on(node) (
              label_replace(cluster:infra_nodes, "label_node_role_kubernetes_io", "infra", "", "")
              * on(node) group_left() max by(node)
              (
                kube_node_status_capacity{resource="cpu",unit="core"}
              )
            )
            or on(node) (
              max without(endpoint, instance, job, pod, service)
              (
                kube_node_labels
              ) * on(node) group_left() max by(node)
              (
                kube_node_status_capacity{resource="cpu",unit="core"}
              )
            )
          )
        record: cluster:capacity_cpu_cores:sum
      - expr: |
          clamp_max(
            label_replace(
              sum by(instance, package, core) (
                node_cpu_info{core!="",package!=""}
                or
                # Assume core = cpu and package = 0 for platforms that don't expose core/package labels.
                label_replace(label_join(node_cpu_info{core="",package=""}, "core", "", "cpu"), "package", "0", "package", "")
              ) > 1,
              "label_node_hyperthread_enabled",
              "true",
              "instance",
              "(.*)"
            ) or on (instance, package)
            label_replace(
              sum by(instance, package, core) (
                label_replace(node_cpu_info{core!="",package!=""}
                or
                # Assume core = cpu and package = 0 for platforms that don't expose core/package labels.
                label_join(node_cpu_info{core="",package=""}, "core", "", "cpu"), "package", "0", "package", "")
              ) <= 1,
              "label_node_hyperthread_enabled",
              "false",
              "instance",
              "(.*)"
            ),
            1
          )
        record: cluster:cpu_core_hyperthreading
      - expr: |
          topk by(node) (1, cluster:nodes_roles) * on (node)
            group_right( label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_node_openshift_io_os_id, label_kubernetes_io_arch,
                         label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra)
          label_replace( cluster:cpu_core_hyperthreading, "node", "$1", "instance", "(.*)" )
        record: cluster:cpu_core_node_labels
      - expr: count(cluster:cpu_core_node_labels) by (label_beta_kubernetes_io_instance_type,
          label_node_hyperthread_enabled)
        record: cluster:capacity_cpu_cores_hyperthread_enabled:sum
      - expr: |
          sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io)
          (
            (
              cluster:master_nodes
              * on(node) group_left() max by(node)
              (
                kube_node_status_capacity{resource="memory",unit="byte"}
              )
            )
            or on(node)
            (
              max without(endpoint, instance, job, pod, service)
              (
                kube_node_labels
              )
              * on(node) group_left() max by(node)
              (
                kube_node_status_capacity{resource="memory",unit="byte"}
              )
            )
          )
        record: cluster:capacity_memory_bytes:sum
      - expr: sum(1 - rate(node_cpu_seconds_total{mode="idle"}[2m]) * on(namespace,
          pod) group_left(node) node_namespace_pod:kube_pod_info:{pod=~"node-exporter.+"})
        record: cluster:cpu_usage_cores:sum
      - expr: sum(node_memory_MemTotal_bytes{job="node-exporter"} - node_memory_MemAvailable_bytes{job="node-exporter"})
        record: cluster:memory_usage_bytes:sum
      - expr: sum(rate(container_cpu_usage_seconds_total{namespace!~"openshift-.+",pod!="",container=""}[5m]))
        record: workload:cpu_usage_cores:sum
      - expr: cluster:cpu_usage_cores:sum - workload:cpu_usage_cores:sum
        record: openshift:cpu_usage_cores:sum
      - expr: sum(container_memory_working_set_bytes{namespace!~"openshift-.+",pod!="",container=""})
        record: workload:memory_usage_bytes:sum
      - expr: cluster:memory_usage_bytes:sum - workload:memory_usage_bytes:sum
        record: openshift:memory_usage_bytes:sum
      - expr: sum(cluster:master_nodes or on(node) kube_node_labels ) BY (label_beta_kubernetes_io_instance_type,
          label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id)
        record: cluster:node_instance_type_count:sum
      - expr: |
          sum by(provisioner) (
            topk by (namespace, persistentvolumeclaim) (
              1, kube_persistentvolumeclaim_resource_requests_storage_bytes
            ) * on(namespace, persistentvolumeclaim) group_right()
            topk by(namespace, persistentvolumeclaim) (
              1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))
            )
          )
        record: cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum
      - expr: (sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master="",label_node_role_kubernetes_io_infra=""}
          or absent(__does_not_exist__)*0)) + ((sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master="true"}
          or absent(__does_not_exist__)*0) * ((max(cluster_master_schedulable == 1)*0+1)
          or (absent(cluster_master_schedulable == 1)*0))))
        record: workload:capacity_physical_cpu_cores:sum
      - expr: min_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])
        record: cluster:usage:workload:capacity_physical_cpu_cores:min:5m
      - expr: max_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])
        record: cluster:usage:workload:capacity_physical_cpu_cores:max:5m
      - expr: |
          sum  by (provisioner) (
            topk by (namespace, persistentvolumeclaim) (
              1, kubelet_volume_stats_used_bytes
            ) * on (namespace,persistentvolumeclaim) group_right()
            topk by (namespace, persistentvolumeclaim) (
              1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))
            )
          )
        record: cluster:kubelet_volume_stats_used_bytes:provisioner:sum
      - expr: sum by (instance) (apiserver_storage_objects != -1)
        record: instance:etcd_object_counts:sum
      - expr: topk(500, max by(resource) (apiserver_storage_objects != -1))
        record: cluster:usage:resources:sum
      - expr: count(count (kube_pod_restart_policy{type!="Always",namespace!~"openshift-.+"})
          by (namespace,pod))
        record: cluster:usage:pods:terminal:workload:sum
      - expr: sum(max(kubelet_containers_per_pod_count_sum) by (instance))
        record: cluster:usage:containers:sum
      - expr: count(cluster:cpu_core_node_labels) by (label_kubernetes_io_arch, label_node_hyperthread_enabled,
          label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
        record: node_role_os_version_machine:cpu_capacity_cores:sum
      - expr: count(max(cluster:cpu_core_node_labels) by (node, package, label_beta_kubernetes_io_instance_type,
          label_node_hyperthread_enabled, label_node_role_kubernetes_io) ) by ( label_beta_kubernetes_io_instance_type,
          label_node_hyperthread_enabled, label_node_role_kubernetes_io)
        record: cluster:capacity_cpu_sockets_hyperthread_enabled:sum
      - expr: count (max(cluster:cpu_core_node_labels) by (node, package, label_kubernetes_io_arch,
          label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
          ) by (label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
        record: node_role_os_version_machine:cpu_capacity_sockets:sum
      - expr: max(alertmanager_integrations{namespace="openshift-monitoring"})
        record: cluster:alertmanager_integrations:max
      - expr: sum by(plugin_name, volume_mode)(pv_collector_total_pv_count{volume_plugin!~".*-e2e-.*"})
        record: cluster:kube_persistentvolume_plugin_type_counts:sum
      - expr: |
          sum(
            min by (node) (kube_node_status_condition{condition="Ready",status="true"})
              and
            max by (node) (kube_node_role{role="master"})
          ) == bool sum(kube_node_role{role="master"})
        record: cluster:control_plane:all_nodes_ready
      - expr: max by (profile) (cluster_monitoring_operator_collection_profile ==
          1)
        record: profile:cluster_monitoring_operator_collection_profile:max
      - alert: ClusterMonitoringOperatorReconciliationErrors
        annotations:
          description: Errors are occurring during reconciliation cycles. Inspect
            the cluster-monitoring-operator log for potential root causes.
          summary: Cluster Monitoring Operator is experiencing unexpected reconciliation
            errors.
        expr: max_over_time(cluster_monitoring_operator_last_reconciliation_successful[5m])
          == 0
        for: 1h
        labels:
          severity: warning
      - alert: ClusterMonitoringOperatorDeprecatedConfig
        annotations:
          description: The configuration field {{ "{{" }} $labels.field {{ "}}" }} in {{ "{{" }} $labels.configmap
            {{ "}}" }} was deprecated in {{ "{{" }} $labels.deprecation_version {{ "}}" }} and has no effect.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/ClusterMonitoringOperatorDeprecatedConfig.md
          summary: Cluster Monitoring Operator is being used with deprecated configuration.
        expr: max by (configmap, field, deprecation_version) (cluster_monitoring_operator_deprecated_config_in_use)
          == 1
        for: 1h
        labels:
          severity: info
      - alert: AlertmanagerReceiversNotConfigured
        annotations:
          description: Alerts are not configured to be sent to a notification system,
            meaning that you may not be notified in a timely fashion when important
            failures occur. Check the OpenShift documentation to learn how to configure
            notifications with Alertmanager.
          summary: Receivers (notification integrations) are not configured on Alertmanager
        expr: cluster:alertmanager_integrations:max == 0
        for: 10m
        labels:
          namespace: cluster-monitoring
          severity: warning
      - alert: KubeDeploymentReplicasMismatch
        annotations:
          description: Deployment {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.deployment {{ "}}" }}
            has not matched the expected number of replicas for longer than 15 minutes.
            This indicates that cluster infrastructure is unable to start or restart
            the necessary components. This most often occurs when one or more nodes
            are down or partioned from the cluster, or a fault occurs on the node
            that prevents the workload from starting. In rare cases this may indicate
            a new version of a cluster component cannot start due to a bug or configuration
            error. Assess the pods for this deployment to verify they are running
            on healthy nodes and then contact support.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeDeploymentReplicasMismatch.md
          summary: Deployment has not matched the expected number of replicas
        expr: |
          (((
            kube_deployment_spec_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              >
            kube_deployment_status_replicas_available{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          ) and (
            changes(kube_deployment_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
              ==
            0
          )) * on() group_left cluster:control_plane:all_nodes_ready) > 0
        for: 15m
        labels:
          severity: warning
      - expr: avg_over_time((((count((max by (node) (up{job="kubelet",metrics_path="/metrics"}
          == 1) and max by (node) (kube_node_status_condition{condition="Ready",status="true"}
          == 1) and min by (node) (kube_node_spec_unschedulable == 0))) / scalar(count(min
          by (node) (kube_node_spec_unschedulable == 0))))))[5m:1s])
        record: cluster:usage:kube_schedulable_node_ready_reachable:avg5m
      - expr: avg_over_time((count(max by (node) (kube_node_status_condition{condition="Ready",status="true"}
          == 1)) / scalar(count(max by (node) (kube_node_status_condition{condition="Ready",status="true"}))))[5m:1s])
        record: cluster:usage:kube_node_ready:avg5m
      - expr: (max without (condition,container,endpoint,instance,job,service) (((kube_pod_status_ready{condition="false"}
          == 1)*0 or (kube_pod_status_ready{condition="true"} == 1)) * on(pod,namespace)
          group_left() group by (pod,namespace) (kube_pod_status_phase{phase=~"Running|Unknown|Pending"}
          == 1)))
        record: kube_running_pod_ready
      - expr: avg(kube_running_pod_ready{namespace=~"openshift-.*"})
        record: cluster:usage:openshift:kube_running_pod_ready:avg
      - expr: avg(kube_running_pod_ready{namespace!~"openshift-.*"})
        record: cluster:usage:workload:kube_running_pod_ready:avg
      - alert: KubePodNotScheduled
        annotations:
          description: |-
            Pod {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }} cannot be scheduled for more than 30 minutes.
            Check the details of the pod with the following command:
            oc describe -n {{ "{{" }} $labels.namespace {{ "}}" }} pod {{ "{{" }} $labels.pod {{ "}}" }}
          summary: Pod cannot be scheduled.
        expr: last_over_time(kube_pod_status_unschedulable{namespace=~"(openshift-.*|kube-.*|default)"}[5m])
          == 1
        for: 30m
        labels:
          severity: warning
    - interval: 30s
      name: kubernetes-recurring.rules
      rules:
      - expr: sum_over_time(workload:capacity_physical_cpu_cores:sum[30s:1s]) + ((cluster:usage:workload:capacity_physical_cpu_core_seconds
          offset 25s) or (absent(cluster:usage:workload:capacity_physical_cpu_core_seconds
          offset 25s)*0))
        record: cluster:usage:workload:capacity_physical_cpu_core_seconds
    - name: openshift-ingress.rules
      rules:
      - expr: sum by (code) (rate(haproxy_server_http_responses_total[5m]) > 0)
        record: code:cluster:ingress_http_request_count:rate5m:sum
      - expr: sum (rate(haproxy_frontend_bytes_in_total[5m]))
        record: cluster:usage:ingress_frontend_bytes_in:rate5m:sum
      - expr: sum (rate(haproxy_frontend_bytes_out_total[5m]))
        record: cluster:usage:ingress_frontend_bytes_out:rate5m:sum
      - expr: sum (haproxy_frontend_current_sessions)
        record: cluster:usage:ingress_frontend_connections:sum
      - expr: sum(max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{code!~"2xx|1xx|4xx|3xx",exported_namespace!~"openshift-.*"}[5m])
          > 0)) / sum (max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{exported_namespace!~"openshift-.*"}[5m])))
          or absent(__does_not_exist__)*0
        record: cluster:usage:workload:ingress_request_error:fraction5m
      - expr: sum (max without(service,endpoint,container,pod,job,namespace) (irate(haproxy_server_http_responses_total{exported_namespace!~"openshift-.*"}[5m])))
          or absent(__does_not_exist__)*0
        record: cluster:usage:workload:ingress_request_total:irate5m
      - expr: sum(max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{code!~"2xx|1xx|4xx|3xx",exported_namespace=~"openshift-.*"}[5m])
          > 0)) / sum (max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{exported_namespace=~"openshift-.*"}[5m])))
          or absent(__does_not_exist__)*0
        record: cluster:usage:openshift:ingress_request_error:fraction5m
      - expr: sum (max without(service,endpoint,container,pod,job,namespace) (irate(haproxy_server_http_responses_total{exported_namespace=~"openshift-.*"}[5m])))
          or absent(__does_not_exist__)*0
        record: cluster:usage:openshift:ingress_request_total:irate5m
      - expr: sum(ingress_controller_aws_nlb_active) or vector(0)
        record: cluster:ingress_controller_aws_nlb_active:sum
    - name: openshift-build.rules
      rules:
      - expr: sum by (strategy) (openshift_build_status_phase_total)
        record: openshift:build_by_strategy:sum
    - name: openshift-monitoring.rules
      rules:
      - expr: sum by (job,namespace) (max without(instance) (prometheus_tsdb_head_series{namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}))
        record: openshift:prometheus_tsdb_head_series:sum
      - expr: sum by(job,namespace) (max without(instance) (rate(prometheus_tsdb_head_samples_appended_total{namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[2m])))
        record: openshift:prometheus_tsdb_head_samples_appended_total:sum
      - expr: sum by (namespace) (max without(instance) (container_memory_working_set_bytes{namespace=~"openshift-monitoring|openshift-user-workload-monitoring",
          container=""}))
        record: monitoring:container_memory_working_set_bytes:sum
      - expr: topk(3, sum by(namespace, job)(sum_over_time(scrape_series_added[1h])))
        record: namespace_job:scrape_series_added:topk3_sum1h
      - expr: topk(3, max by(namespace, job) (topk by(namespace,job) (1, scrape_samples_post_metric_relabeling)))
        record: namespace_job:scrape_samples_post_metric_relabeling:topk3
      - expr: sum by(exported_service) (rate(haproxy_server_http_responses_total{exported_namespace="openshift-monitoring",
          exported_service=~"alertmanager-main|prometheus-k8s"}[5m]))
        record: monitoring:haproxy_server_http_responses_total:sum
      - expr: max by (cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job="kube-state-metrics",
          owner_kind="ReplicationController"},"replicationcontroller", "$1", "owner_name",
          "(.*)") * on(replicationcontroller, namespace) group_left(owner_name) topk
          by(replicationcontroller, namespace) (1, max by (replicationcontroller,
          namespace, owner_name) (kube_replicationcontroller_owner{job="kube-state-metrics"})),"workload",
          "$1", "owner_name", "(.*)"))
        labels:
          workload_type: deploymentconfig
        record: namespace_workload_pod:kube_pod_owner:relabel
    - name: openshift-etcd-telemetry.rules
      rules:
      - expr: sum by (instance) (etcd_mvcc_db_total_size_in_bytes{job="etcd"})
        record: instance:etcd_mvcc_db_total_size_in_bytes:sum
      - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_wal_fsync_duration_seconds_bucket{job="etcd"}[5m])))
        labels:
          quantile: "0.99"
        record: instance:etcd_disk_wal_fsync_duration_seconds:histogram_quantile
      - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_network_peer_round_trip_time_seconds_bucket{job="etcd"}[5m])))
        labels:
          quantile: "0.99"
        record: instance:etcd_network_peer_round_trip_time_seconds:histogram_quantile
      - expr: sum by (instance) (etcd_mvcc_db_total_size_in_use_in_bytes{job="etcd"})
        record: instance:etcd_mvcc_db_total_size_in_use_in_bytes:sum
      - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_backend_commit_duration_seconds_bucket{job="etcd"}[5m])))
        labels:
          quantile: "0.99"
        record: instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile
    - name: openshift-sre.rules
      rules:
      - expr: sum(rate(apiserver_request_total{job="apiserver"}[10m])) BY (code)
        record: code:apiserver_request_total:rate:sum
    - name: apiserver-list-watch.rules
      rules:
      - expr: sum by(verb) (rate(apiserver_request_total{verb=~"LIST|WATCH",code=~"2.."}[5m]))
        record: apiserver_list_watch_request_success_total:rate:sum
    - name: general.rules
      rules:
      - alert: Watchdog
        annotations:
          description: |
            This is an alert meant to ensure that the entire alerting pipeline is functional.
            This alert is always firing, therefore it should always be firing in Alertmanager
            and always fire against a receiver. There are integrations with various notification
            mechanisms that send a notification when this alert is not firing. For example the
            "DeadMansSnitch" integration in PagerDuty.
          summary: An alert that should always be firing to certify that Alertmanager
            is working properly.
        expr: vector(1)
        labels:
          namespace: cluster-monitoring
          severity: none
    - name: node-network
      rules:
      - alert: NodeNetworkInterfaceFlapping
        annotations:
          description: Network interface "{{ "{{" }} $labels.device {{ "}}" }}" changing its up status
            often on node-exporter {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }}
          summary: Network interface is often changing its status
        expr: |
          changes(node_network_up{job="node-exporter",device!~"veth.+|tunbr"}[2m]) > 2
        for: 2m
        labels:
          severity: warning
    - name: kube-prometheus-node-recording.rules
      rules:
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[3m]))
          BY (instance)
        record: instance:node_cpu:rate:sum
      - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
        record: instance:node_network_receive_bytes:rate:sum
      - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
        record: instance:node_network_transmit_bytes:rate:sum
      - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m]))
        record: cluster:node_cpu:sum_rate5m
      - expr: cluster:node_cpu:sum_rate5m / count(sum(node_cpu_seconds_total) BY (instance,
          cpu))
        record: cluster:node_cpu:ratio
    - name: kube-prometheus-general.rules
      rules:
      - expr: count without(instance, pod, node) (up == 1)
        record: count:up1
      - expr: count without(instance, pod, node) (up == 0)
        record: count:up0
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    labels:
      app.kubernetes.io/component: exporter
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: kube-state-metrics
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 2.15.0
      prometheus: k8s
      role: alert-rules
    name: kube-state-metrics-rules
    namespace: cluster-monitoring
  spec:
    groups:
    - name: kube-state-metrics
      rules:
      - alert: KubeStateMetricsListErrors
        annotations:
          description: kube-state-metrics is experiencing errors at an elevated rate
            in list operations. This is likely causing it to not be able to expose
            metrics about Kubernetes objects correctly or at all.
          summary: kube-state-metrics is experiencing errors in list operations.
        expr: |
          (sum(rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
            /
          sum(rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])) by (cluster))
          > 0.01
        for: 15m
        labels:
          namespace: cluster-monitoring
          severity: warning
      - alert: KubeStateMetricsWatchErrors
        annotations:
          description: kube-state-metrics is experiencing errors at an elevated rate
            in watch operations. This is likely causing it to not be able to expose
            metrics about Kubernetes objects correctly or at all.
          summary: kube-state-metrics is experiencing errors in watch operations.
        expr: |
          (sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
            /
          sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])) by (cluster))
          > 0.01
        for: 15m
        labels:
          namespace: cluster-monitoring
          severity: warning
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    labels:
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: kube-prometheus
      app.kubernetes.io/part-of: openshift-monitoring
      prometheus: k8s
      role: alert-rules
    name: kubernetes-monitoring-rules
    namespace: cluster-monitoring
  spec:
    groups:
    - name: kubernetes-apps
      rules:
      - alert: KubePodCrashLooping
        annotations:
          description: 'Pod {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }} ({{ "{{" }} $labels.container
            {{ "}}" }}) is in waiting state (reason: "CrashLoopBackOff").'
          summary: Pod is crash looping.
        expr: |
          max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m]) >= 1
        for: 15m
        labels:
          severity: warning
      - alert: KubePodNotReady
        annotations:
          description: Pod {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.pod {{ "}}" }} has been in a
            non-ready state for longer than 15 minutes.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePodNotReady.md
          summary: Pod has been in a non-ready state for more than 15 minutes.
        expr: |
          sum by (namespace, pod, cluster) (
            max by(namespace, pod, cluster) (
              kube_pod_status_phase{namespace=~"(openshift-.*|kube-.*|default)", job="kube-state-metrics", phase=~"Pending|Unknown"}
              unless ignoring(phase) (kube_pod_status_unschedulable{job="kube-state-metrics"} == 1)
            ) * on(namespace, pod, cluster) group_left(owner_kind) topk by(namespace, pod, cluster) (
              1, max by(namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"})
            )
          ) > 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeDeploymentGenerationMismatch
        annotations:
          description: Deployment generation for {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.deployment
            {{ "}}" }} does not match, this indicates that the Deployment has failed but has
            not been rolled back.
          summary: Deployment generation mismatch due to possible roll-back
        expr: |
          kube_deployment_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            !=
          kube_deployment_metadata_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
        for: 15m
        labels:
          severity: warning
      - alert: KubeDeploymentRolloutStuck
        annotations:
          description: Rollout of deployment {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.deployment
            {{ "}}" }} is not progressing for longer than 15 minutes.
          summary: Deployment rollout is not progressing.
        expr: |
          kube_deployment_status_condition{condition="Progressing", status="false",namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          != 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeStatefulSetReplicasMismatch
        annotations:
          description: StatefulSet {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.statefulset
            {{ "}}" }} has not matched the expected number of replicas for longer than 15
            minutes.
          summary: StatefulSet has not matched the expected number of replicas.
        expr: |
          (
            kube_statefulset_status_replicas_ready{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              !=
            kube_statefulset_status_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          ) and (
            changes(kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[10m])
              ==
            0
          )
        for: 15m
        labels:
          severity: warning
      - alert: KubeStatefulSetGenerationMismatch
        annotations:
          description: StatefulSet generation for {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.statefulset
            {{ "}}" }} does not match, this indicates that the StatefulSet has failed but
            has not been rolled back.
          summary: StatefulSet generation mismatch due to possible roll-back
        expr: |
          kube_statefulset_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            !=
          kube_statefulset_metadata_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
        for: 15m
        labels:
          severity: warning
      - alert: KubeStatefulSetUpdateNotRolledOut
        annotations:
          description: StatefulSet {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.statefulset
            {{ "}}" }} update has not been rolled out.
          summary: StatefulSet update has not been rolled out.
        expr: |
          (
            max by(namespace, statefulset, job, cluster) (
              kube_statefulset_status_current_revision{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
                unless
              kube_statefulset_status_update_revision{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            )
              *
            (
              kube_statefulset_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
                !=
              kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            )
          )  and (
            changes(kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
              ==
            0
          )
        for: 15m
        labels:
          severity: warning
      - alert: KubeDaemonSetRolloutStuck
        annotations:
          description: DaemonSet {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.daemonset {{ "}}" }} has
            not finished or progressed for at least 30 minutes.
          summary: DaemonSet rollout is stuck.
        expr: |
          (
            (
              kube_daemonset_status_current_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
               !=
              kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            ) or (
              kube_daemonset_status_number_misscheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
               !=
              0
            ) or (
              kube_daemonset_status_updated_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
               !=
              kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            ) or (
              kube_daemonset_status_number_available{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
               !=
              kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            )
          ) and (
            changes(kube_daemonset_status_updated_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
              ==
            0
          )
        for: 30m
        labels:
          severity: warning
      - alert: KubeContainerWaiting
        annotations:
          description: 'pod/{{ "{{" }} $labels.pod {{ "}}" }} in namespace {{ "{{" }} $labels.namespace {{ "}}" }}
            on container {{ "{{" }} $labels.container{{ "}}" }} has been in waiting state for longer
            than 1 hour. (reason: "{{ "{{" }} $labels.reason {{ "}}" }}").'
          summary: Pod container waiting longer than 1 hour
        expr: |
          kube_pod_container_status_waiting_reason{reason!="CrashLoopBackOff", namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
        for: 1h
        labels:
          severity: warning
      - alert: KubeDaemonSetNotScheduled
        annotations:
          description: '{{ "{{" }} $value {{ "}}" }} Pods of DaemonSet {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }}
            $labels.daemonset {{ "}}" }} are not scheduled.'
          summary: DaemonSet pods are not scheduled.
        expr: |
          kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            -
          kube_daemonset_status_current_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
        for: 10m
        labels:
          severity: warning
      - alert: KubeDaemonSetMisScheduled
        annotations:
          description: '{{ "{{" }} $value {{ "}}" }} Pods of DaemonSet {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }}
            $labels.daemonset {{ "}}" }} are running where they are not supposed to run.'
          summary: DaemonSet pods are misscheduled.
        expr: |
          kube_daemonset_status_number_misscheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeJobNotCompleted
        annotations:
          description: Job {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.job_name {{ "}}" }} is taking
            more than {{ "{{" }} "43200" | humanizeDuration {{ "}}" }} to complete.
          summary: Job did not complete in time
        expr: |
          time() - max by(namespace, job_name, cluster) (kube_job_status_start_time{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            and
          kube_job_status_active{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0) > 43200
        labels:
          severity: warning
      - alert: KubeJobFailed
        annotations:
          description: Job {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.job_name {{ "}}" }} failed to
            complete. Removing failed job after investigation should clear this alert.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeJobFailed.md
          summary: Job failed to complete.
        expr: |
          kube_job_failed{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}  > 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeHpaReplicasMismatch
        annotations:
          description: HPA {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.horizontalpodautoscaler  {{ "}}" }}
            has not matched the desired number of replicas for longer than 15 minutes.
          summary: HPA has not matched desired number of replicas.
        expr: |
          (kube_horizontalpodautoscaler_status_desired_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            !=
          kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
            and
          (kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            >
          kube_horizontalpodautoscaler_spec_min_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
            and
          (kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            <
          kube_horizontalpodautoscaler_spec_max_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
            and
          changes(kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[15m]) == 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeHpaMaxedOut
        annotations:
          description: HPA {{ "{{" }} $labels.namespace {{ "}}" }}/{{ "{{" }} $labels.horizontalpodautoscaler  {{ "}}" }}
            has been running at max replicas for longer than 15 minutes.
          summary: HPA is running at max replicas
        expr: |
          kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            ==
          kube_horizontalpodautoscaler_spec_max_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
        for: 15m
        labels:
          severity: warning
    - name: kubernetes-resources
      rules:
      - alert: KubeCPUOvercommit
        annotations:
          description: Cluster {{ "{{" }} $labels.cluster {{ "}}" }} has overcommitted CPU resource
            requests for Pods by {{ "{{" }} $value {{ "}}" }} CPU shares and cannot tolerate node
            failure.
          summary: Cluster has overcommitted CPU resource requests.
        expr: |
          sum(namespace_cpu:kube_pod_container_resource_requests:sum{}) by (cluster) - (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
          and
          (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
        for: 10m
        labels:
          namespace: kube-system
          severity: warning
      - alert: KubeMemoryOvercommit
        annotations:
          description: Cluster {{ "{{" }} $labels.cluster {{ "}}" }} has overcommitted memory resource
            requests for Pods by {{ "{{" }} $value | humanize {{ "}}" }} bytes and cannot tolerate
            node failure.
          summary: Cluster has overcommitted memory resource requests.
        expr: |
          sum(namespace_memory:kube_pod_container_resource_requests:sum{}) by (cluster) - (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
          and
          (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
        for: 10m
        labels:
          namespace: kube-system
          severity: warning
      - alert: KubeQuotaAlmostFull
        annotations:
          description: Namespace {{ "{{" }} $labels.namespace {{ "}}" }} is using {{ "{{" }} $value | humanizePercentage
            {{ "}}" }} of its {{ "{{" }} $labels.resource {{ "}}" }} quota.
          summary: Namespace quota is going to be full.
        expr: |
          kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
            / ignoring(instance, job, type)
          (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
            > 0.9 < 1
        for: 15m
        labels:
          severity: info
      - alert: KubeQuotaFullyUsed
        annotations:
          description: Namespace {{ "{{" }} $labels.namespace {{ "}}" }} is using {{ "{{" }} $value | humanizePercentage
            {{ "}}" }} of its {{ "{{" }} $labels.resource {{ "}}" }} quota.
          summary: Namespace quota is fully used.
        expr: |
          kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
            / ignoring(instance, job, type)
          (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
            == 1
        for: 15m
        labels:
          severity: info
      - alert: KubeQuotaExceeded
        annotations:
          description: Namespace {{ "{{" }} $labels.namespace {{ "}}" }} is using {{ "{{" }} $value | humanizePercentage
            {{ "}}" }} of its {{ "{{" }} $labels.resource {{ "}}" }} quota.
          summary: Namespace quota has exceeded the limits.
        expr: |
          kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
            / ignoring(instance, job, type)
          (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
            > 1
        for: 15m
        labels:
          severity: warning
    - name: kubernetes-system
      rules:
      - alert: KubeClientErrors
        annotations:
          description: Kubernetes API server client '{{ "{{" }} $labels.job {{ "}}" }}/{{ "{{" }} $labels.instance
            {{ "}}" }}' is experiencing {{ "{{" }} $value | humanizePercentage {{ "}}" }} errors.'
          summary: Kubernetes API server client is experiencing errors.
        expr: |
          (sum(rate(rest_client_requests_total{job="apiserver",code=~"5.."}[5m])) by (cluster, instance, job, namespace)
            /
          sum(rate(rest_client_requests_total{job="apiserver"}[5m])) by (cluster, instance, job, namespace))
          > 0.01
        for: 15m
        labels:
          severity: warning
    - name: kubernetes-system-kubelet
      rules:
      - alert: KubeNodeNotReady
        annotations:
          description: '{{ "{{" }} $labels.node {{ "}}" }} has been unready for more than 15 minutes.'
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeNodeNotReady.md
          summary: Node is not ready.
        expr: |
          kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeNodeUnreachable
        annotations:
          description: '{{ "{{" }} $labels.node {{ "}}" }} is unreachable and some workloads may be
            rescheduled.'
          summary: Node is unreachable.
        expr: |
          (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}) == 1
        for: 15m
        labels:
          severity: warning
      - alert: KubeletTooManyPods
        annotations:
          description: Kubelet '{{ "{{" }} $labels.node {{ "}}" }}' is running at {{ "{{" }} $value | humanizePercentage
            {{ "}}" }} of its Pod capacity.
          summary: Kubelet is running at capacity.
        expr: |
          count by(cluster, node) (
            (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
          )
          /
          max by(cluster, node) (
            kube_node_status_capacity{job="kube-state-metrics",resource="pods"} != 1
          ) > 0.95
        for: 15m
        labels:
          namespace: kube-system
          severity: info
      - alert: KubeNodeReadinessFlapping
        annotations:
          description: The readiness status of node {{ "{{" }} $labels.node {{ "}}" }} has changed
            {{ "{{" }} $value {{ "}}" }} times in the last 15 minutes.
          summary: Node readiness status is flapping.
        expr: |
          sum(changes(kube_node_status_condition{job="kube-state-metrics",status="true",condition="Ready"}[15m])) by (cluster, node) > 2
        for: 15m
        labels:
          namespace: kube-system
          severity: warning
      - alert: KubeletPlegDurationHigh
        annotations:
          description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile
            duration of {{ "{{" }} $value {{ "}}" }} seconds on node {{ "{{" }} $labels.node {{ "}}" }}.
          summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
        expr: |
          node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
        for: 5m
        labels:
          namespace: kube-system
          severity: warning
      - alert: KubeletPodStartUpLatencyHigh
        annotations:
          description: Kubelet Pod startup 99th percentile latency is {{ "{{" }} $value {{ "}}" }}
            seconds on node {{ "{{" }} $labels.node {{ "}}" }}.
          summary: Kubelet Pod startup latency is too high.
        expr: |
          histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le)) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"} > 60
        for: 15m
        labels:
          namespace: kube-system
          severity: warning
      - alert: KubeletClientCertificateRenewalErrors
        annotations:
          description: Kubelet on node {{ "{{" }} $labels.node {{ "}}" }} has failed to renew its
            client certificate ({{ "{{" }} $value | humanize {{ "}}" }} errors in the last 5 minutes).
          summary: Kubelet has failed to renew its client certificate.
        expr: |
          increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeletServerCertificateRenewalErrors
        annotations:
          description: Kubelet on node {{ "{{" }} $labels.node {{ "}}" }} has failed to renew its
            server certificate ({{ "{{" }} $value | humanize {{ "}}" }} errors in the last 5 minutes).
          summary: Kubelet has failed to renew its server certificate.
        expr: |
          increase(kubelet_server_expiration_renew_errors[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: KubeletDown
        annotations:
          description: Kubelet has disappeared from Prometheus target discovery.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeletDown.md
          summary: Target disappeared from Prometheus target discovery.
        expr: |
          absent(up{job="kubelet", metrics_path="/metrics"} == 1)
        for: 15m
        labels:
          namespace: kube-system
          severity: critical
    - name: k8s.rules.container_cpu_usage_seconds_total
      rules:
      - expr: |
          sum by (cluster, namespace, pod, container) (
            irate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
          ) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
            1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
    - name: k8s.rules.container_memory_working_set_bytes
      rules:
      - expr: |
          container_memory_working_set_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
            max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_working_set_bytes
    - name: k8s.rules.container_memory_rss
      rules:
      - expr: |
          container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
            max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_rss
    - name: k8s.rules.container_memory_cache
      rules:
      - expr: |
          container_memory_cache{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
            max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_cache
    - name: k8s.rules.container_memory_swap
      rules:
      - expr: |
          container_memory_swap{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
          * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
            max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
          )
        record: node_namespace_pod_container:container_memory_swap
    - name: k8s.rules.container_memory_requests
      rules:
      - expr: |
          kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
          group_left() max by (namespace, pod, cluster) (
            (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
          )
        record: cluster:namespace:pod_memory:active:kube_pod_container_resource_requests
      - expr: |
          sum by (namespace, cluster) (
              sum by (namespace, pod, cluster) (
                  max by (namespace, pod, container, cluster) (
                    kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}
                  ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                    kube_pod_status_phase{phase=~"Pending|Running"} == 1
                  )
              )
          )
        record: namespace_memory:kube_pod_container_resource_requests:sum
    - name: k8s.rules.container_cpu_requests
      rules:
      - expr: |
          kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
          group_left() max by (namespace, pod, cluster) (
            (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
          )
        record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests
      - expr: |
          sum by (namespace, cluster) (
              sum by (namespace, pod, cluster) (
                  max by (namespace, pod, container, cluster) (
                    kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}
                  ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                    kube_pod_status_phase{phase=~"Pending|Running"} == 1
                  )
              )
          )
        record: namespace_cpu:kube_pod_container_resource_requests:sum
    - name: k8s.rules.container_memory_limits
      rules:
      - expr: |
          kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
          group_left() max by (namespace, pod, cluster) (
            (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
          )
        record: cluster:namespace:pod_memory:active:kube_pod_container_resource_limits
      - expr: |
          sum by (namespace, cluster) (
              sum by (namespace, pod, cluster) (
                  max by (namespace, pod, container, cluster) (
                    kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}
                  ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                    kube_pod_status_phase{phase=~"Pending|Running"} == 1
                  )
              )
          )
        record: namespace_memory:kube_pod_container_resource_limits:sum
    - name: k8s.rules.container_cpu_limits
      rules:
      - expr: |
          kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
          group_left() max by (namespace, pod, cluster) (
           (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
           )
        record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits
      - expr: |
          sum by (namespace, cluster) (
              sum by (namespace, pod, cluster) (
                  max by (namespace, pod, container, cluster) (
                    kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}
                  ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                    kube_pod_status_phase{phase=~"Pending|Running"} == 1
                  )
              )
          )
        record: namespace_cpu:kube_pod_container_resource_limits:sum
    - name: k8s.rules.pod_owner
      rules:
      - expr: |
          max by (cluster, namespace, workload, pod) (
            label_replace(
              label_replace(
                kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
                "replicaset", "$1", "owner_name", "(.*)"
              ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (
                1, max by (replicaset, namespace, owner_name) (
                  kube_replicaset_owner{job="kube-state-metrics"}
                )
              ),
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: deployment
        record: namespace_workload_pod:kube_pod_owner:relabel
      - expr: |
          max by (cluster, namespace, workload, pod) (
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: daemonset
        record: namespace_workload_pod:kube_pod_owner:relabel
      - expr: |
          max by (cluster, namespace, workload, pod) (
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: statefulset
        record: namespace_workload_pod:kube_pod_owner:relabel
      - expr: |
          max by (cluster, namespace, workload, pod) (
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="Job"},
              "workload", "$1", "owner_name", "(.*)"
            )
          )
        labels:
          workload_type: job
        record: namespace_workload_pod:kube_pod_owner:relabel
    - name: kube-scheduler.rules
      rules:
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.99"
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.9"
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
        labels:
          quantile: "0.5"
        record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
    - name: node.rules
      rules:
      - expr: |
          topk by(cluster, namespace, pod) (1,
            max by (cluster, node, namespace, pod) (
              label_replace(kube_pod_info{job="kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
          ))
        record: 'node_namespace_pod:kube_pod_info:'
      - expr: |
          sum(
            node_memory_MemAvailable_bytes{job="node-exporter"} or
            (
              node_memory_Buffers_bytes{job="node-exporter"} +
              node_memory_Cached_bytes{job="node-exporter"} +
              node_memory_MemFree_bytes{job="node-exporter"} +
              node_memory_Slab_bytes{job="node-exporter"}
            )
          ) by (cluster)
        record: :node_memory_MemAvailable_bytes:sum
      - expr: |
          avg by (cluster, node) (
            sum without (mode) (
              rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal",job="node-exporter"}[5m])
            )
          )
        record: node:node_cpu_utilization:ratio_rate5m
      - expr: |
          avg by (cluster) (
            node:node_cpu_utilization:ratio_rate5m
          )
        record: cluster:node_cpu:ratio_rate5m
    - name: kubelet.rules
      rules:
      - expr: |
          histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
        labels:
          quantile: "0.99"
        record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
        labels:
          quantile: "0.9"
        record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
      - expr: |
          histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
        labels:
          quantile: "0.5"
        record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    labels:
      app.kubernetes.io/component: exporter
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: node-exporter
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 1.9.1
      prometheus: k8s
      role: alert-rules
    name: node-exporter-rules
    namespace: cluster-monitoring
  spec:
    groups:
    - name: node-exporter
      rules:
      - alert: NodeFilesystemSpaceFillingUp
        annotations:
          description: Filesystem on {{ "{{" }} $labels.device {{ "}}" }}, mounted on {{ "{{" }} $labels.mountpoint
            {{ "}}" }}, at {{ "{{" }} $labels.instance {{ "}}" }} has only {{ "{{" }} printf "%.2f" $value {{ "}}" }}% available
            space left and is filling up.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md
          summary: Filesystem is predicted to run out of space within the next 24
            hours.
        expr: |
          (
            node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 15
          and
            predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 24*60*60) < 0
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 1h
        labels:
          severity: warning
      - alert: NodeFilesystemSpaceFillingUp
        annotations:
          description: Filesystem on {{ "{{" }} $labels.device {{ "}}" }}, mounted on {{ "{{" }} $labels.mountpoint
            {{ "}}" }}, at {{ "{{" }} $labels.instance {{ "}}" }} has only {{ "{{" }} printf "%.2f" $value {{ "}}" }}% available
            space left and is filling up fast.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md
          summary: Filesystem is predicted to run out of space within the next 4 hours.
        expr: |
          (
            node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 10
          and
            predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 4*60*60) < 0
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 1h
        labels:
          severity: critical
      - alert: NodeFilesystemAlmostOutOfSpace
        annotations:
          description: Filesystem on {{ "{{" }} $labels.device {{ "}}" }}, mounted on {{ "{{" }} $labels.mountpoint
            {{ "}}" }}, at {{ "{{" }} $labels.instance {{ "}}" }} has only {{ "{{" }} printf "%.2f" $value {{ "}}" }}% available
            space left.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md
          summary: Filesystem has less than 5% space left.
        expr: |
          (
            node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 5
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 30m
        labels:
          severity: warning
      - alert: NodeFilesystemAlmostOutOfSpace
        annotations:
          description: Filesystem on {{ "{{" }} $labels.device {{ "}}" }}, mounted on {{ "{{" }} $labels.mountpoint
            {{ "}}" }}, at {{ "{{" }} $labels.instance {{ "}}" }} has only {{ "{{" }} printf "%.2f" $value {{ "}}" }}% available
            space left.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md
          summary: Filesystem has less than 3% space left.
        expr: |
          (
            node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 3
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 30m
        labels:
          severity: critical
      - alert: NodeFilesystemFilesFillingUp
        annotations:
          description: Filesystem on {{ "{{" }} $labels.device {{ "}}" }}, mounted on {{ "{{" }} $labels.mountpoint
            {{ "}}" }}, at {{ "{{" }} $labels.instance {{ "}}" }} has only {{ "{{" }} printf "%.2f" $value {{ "}}" }}% available
            inodes left and is filling up.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md
          summary: Filesystem is predicted to run out of inodes within the next 24
            hours.
        expr: |
          (
            node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 40
          and
            predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 24*60*60) < 0
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 1h
        labels:
          severity: warning
      - alert: NodeFilesystemFilesFillingUp
        annotations:
          description: Filesystem on {{ "{{" }} $labels.device {{ "}}" }}, mounted on {{ "{{" }} $labels.mountpoint
            {{ "}}" }}, at {{ "{{" }} $labels.instance {{ "}}" }} has only {{ "{{" }} printf "%.2f" $value {{ "}}" }}% available
            inodes left and is filling up fast.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md
          summary: Filesystem is predicted to run out of inodes within the next 4
            hours.
        expr: |
          (
            node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 20
          and
            predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 4*60*60) < 0
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 1h
        labels:
          severity: critical
      - alert: NodeFilesystemAlmostOutOfFiles
        annotations:
          description: Filesystem on {{ "{{" }} $labels.device {{ "}}" }}, mounted on {{ "{{" }} $labels.mountpoint
            {{ "}}" }}, at {{ "{{" }} $labels.instance {{ "}}" }} has only {{ "{{" }} printf "%.2f" $value {{ "}}" }}% available
            inodes left.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md
          summary: Filesystem has less than 5% inodes left.
        expr: |
          (
            node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 5
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 1h
        labels:
          severity: warning
      - alert: NodeFilesystemAlmostOutOfFiles
        annotations:
          description: Filesystem on {{ "{{" }} $labels.device {{ "}}" }}, mounted on {{ "{{" }} $labels.mountpoint
            {{ "}}" }}, at {{ "{{" }} $labels.instance {{ "}}" }} has only {{ "{{" }} printf "%.2f" $value {{ "}}" }}% available
            inodes left.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md
          summary: Filesystem has less than 3% inodes left.
        expr: |
          (
            node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 3
          and
            node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
          )
        for: 1h
        labels:
          severity: critical
      - alert: NodeNetworkReceiveErrs
        annotations:
          description: '{{ "{{" }} $labels.instance {{ "}}" }} interface {{ "{{" }} $labels.device {{ "}}" }} has
            encountered {{ "{{" }} printf "%.0f" $value {{ "}}" }} receive errors in the last two
            minutes.'
          summary: Network interface is reporting many receive errors.
        expr: |
          rate(node_network_receive_errs_total{job="node-exporter"}[2m]) / rate(node_network_receive_packets_total{job="node-exporter"}[2m]) > 0.01
        for: 1h
        labels:
          severity: warning
      - alert: NodeNetworkTransmitErrs
        annotations:
          description: '{{ "{{" }} $labels.instance {{ "}}" }} interface {{ "{{" }} $labels.device {{ "}}" }} has
            encountered {{ "{{" }} printf "%.0f" $value {{ "}}" }} transmit errors in the last two
            minutes.'
          summary: Network interface is reporting many transmit errors.
        expr: |
          rate(node_network_transmit_errs_total{job="node-exporter"}[2m]) / rate(node_network_transmit_packets_total{job="node-exporter"}[2m]) > 0.01
        for: 1h
        labels:
          severity: warning
      - alert: NodeHighNumberConntrackEntriesUsed
        annotations:
          description: '{{ "{{" }} $value | humanizePercentage {{ "}}" }} of conntrack entries are
            used.'
          summary: Number of conntrack are getting close to the limit.
        expr: |
          (node_nf_conntrack_entries{job="node-exporter"} / node_nf_conntrack_entries_limit) > 0.75
        labels:
          severity: warning
      - alert: NodeTextFileCollectorScrapeError
        annotations:
          description: Node Exporter text file collector on {{ "{{" }} $labels.instance {{ "}}" }}
            failed to scrape.
          summary: Node Exporter text file collector failed to scrape.
        expr: |
          node_textfile_scrape_error{job="node-exporter"} == 1
        labels:
          severity: warning
      - alert: NodeClockSkewDetected
        annotations:
          description: Clock at {{ "{{" }} $labels.instance {{ "}}" }} is out of sync by more than
            0.05s. Ensure NTP is configured correctly on this host.
          summary: Clock skew detected.
        expr: |-
          (
          (
            node_timex_offset_seconds{job="node-exporter"} > 0.05
          and
            deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) >= 0
          )
          or
          (
            node_timex_offset_seconds{job="node-exporter"} < -0.05
          and
            deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) <= 0
          )
          ) and on() absent(up{job="ptp-monitor-service"})
        for: 10m
        labels:
          severity: warning
      - alert: NodeClockNotSynchronising
        annotations:
          description: Clock at {{ "{{" }} $labels.instance {{ "}}" }} is not synchronising. Ensure
            NTP is configured on this host.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeClockNotSynchronising.md
          summary: Clock not synchronising.
        expr: |-
          (
          min_over_time(node_timex_sync_status{job="node-exporter"}[5m]) == 0
          and
          node_timex_maxerror_seconds{job="node-exporter"} >= 16
          ) and on() absent(up{job="ptp-monitor-service"})
        for: 10m
        labels:
          severity: critical
      - alert: NodeFileDescriptorLimit
        annotations:
          description: File descriptors limit at {{ "{{" }} $labels.instance {{ "}}" }} is currently
            at {{ "{{" }} printf "%.2f" $value {{ "}}" }}%.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md
          summary: Kernel is predicted to exhaust file descriptors limit soon.
        expr: |
          (
            node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 70
          )
        for: 15m
        labels:
          severity: warning
      - alert: NodeFileDescriptorLimit
        annotations:
          description: File descriptors limit at {{ "{{" }} $labels.instance {{ "}}" }} is currently
            at {{ "{{" }} printf "%.2f" $value {{ "}}" }}%.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md
          summary: Kernel is predicted to exhaust file descriptors limit soon.
        expr: |
          (
            node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 90
          )
        for: 15m
        labels:
          severity: critical
      - alert: NodeSystemSaturation
        annotations:
          description: |
            System load per core at {{ "{{" }} $labels.instance {{ "}}" }} has been above 2 for the last 15 minutes, is currently at {{ "{{" }} printf "%.2f" $value {{ "}}" }}.
            This might indicate this instance resources saturation and can cause it becoming unresponsive.
          summary: System saturated, load per core is very high.
        expr: |
          node_load1{job="node-exporter"}
          / count without (cpu, mode) (node_cpu_seconds_total{job="node-exporter", mode="idle"}) > 2
        for: 15m
        labels:
          severity: warning
      - alert: NodeMemoryMajorPagesFaults
        annotations:
          description: |
            Memory major pages are occurring at very high rate at {{ "{{" }} $labels.instance {{ "}}" }}, 500 major page faults per second for the last 15 minutes, is currently at {{ "{{" }} printf "%.2f" $value {{ "}}" }}.
            Please check that there is enough memory available at this instance.
          summary: Memory major page faults are occurring at very high rate.
        expr: |
          rate(node_vmstat_pgmajfault{job="node-exporter"}[5m]) > 500
        for: 15m
        labels:
          severity: warning
      - alert: NodeSystemdServiceFailed
        annotations:
          description: Systemd service {{ "{{" }} $labels.name {{ "}}" }} has entered failed state
            at {{ "{{" }} $labels.instance {{ "}}" }}
          summary: Systemd service has entered failed state.
        expr: |
          node_systemd_unit_state{job="node-exporter", state="failed"} == 1
        for: 15m
        labels:
          severity: warning
    - name: node-exporter.rules
      rules:
      - expr: |
          count without (cpu, mode) (
            node_cpu_seconds_total{job="node-exporter",mode="idle"}
          )
        record: instance:node_num_cpu:sum
      - expr: |
          1 - avg without (cpu) (
            sum without (mode) (rate(node_cpu_seconds_total{job="node-exporter", mode=~"idle|iowait|steal"}[1m]))
          )
        record: instance:node_cpu_utilisation:rate1m
      - expr: |
          (
            node_load1{job="node-exporter"}
          /
            instance:node_num_cpu:sum{job="node-exporter"}
          )
        record: instance:node_load1_per_cpu:ratio
      - expr: |
          1 - (
            (
              node_memory_MemAvailable_bytes{job="node-exporter"}
              or
              (
                node_memory_Buffers_bytes{job="node-exporter"}
                +
                node_memory_Cached_bytes{job="node-exporter"}
                +
                node_memory_MemFree_bytes{job="node-exporter"}
                +
                node_memory_Slab_bytes{job="node-exporter"}
              )
            )
          /
            node_memory_MemTotal_bytes{job="node-exporter"}
          )
        record: instance:node_memory_utilisation:ratio
      - expr: |
          rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
        record: instance:node_vmstat_pgmajfault:rate1m
      - expr: |
          rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
        record: instance_device:node_disk_io_time_seconds:rate1m
      - expr: |
          rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
        record: instance_device:node_disk_io_time_weighted_seconds:rate1m
      - expr: |
          sum without (device) (
            rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_receive_bytes_excluding_lo:rate1m
      - expr: |
          sum without (device) (
            rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_transmit_bytes_excluding_lo:rate1m
      - expr: |
          sum without (device) (
            rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_receive_drop_excluding_lo:rate1m
      - expr: |
          sum without (device) (
            rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
          )
        record: instance:node_network_transmit_drop_excluding_lo:rate1m
    - name: telemetry
      rules:
      - expr: sum by(vendor,model) (node_accelerator_card_info)
        record: vendor_model:node_accelerator_cards:sum
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    labels:
      app.kubernetes.io/component: prometheus
      app.kubernetes.io/instance: k8s
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 3.2.1
      prometheus: k8s
      role: alert-rules
    name: prometheus-k8s-prometheus-rules
    namespace: cluster-monitoring
  spec:
    groups:
    - name: prometheus
      rules:
      - alert: PrometheusBadConfig
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} has failed
            to reload its configuration.
          summary: Failed Prometheus configuration reload.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max_over_time(prometheus_config_last_reload_successful{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) == 0
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusSDRefreshFailure
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} has failed
            to refresh SD with mechanism {{ "{{" }}$labels.mechanism{{ "}}" }}.
          summary: Failed Prometheus SD refresh.
        expr: |
          increase(prometheus_sd_refresh_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[10m]) > 0
        for: 20m
        labels:
          severity: warning
      - alert: PrometheusKubernetesListWatchFailures
        annotations:
          description: Kubernetes service discovery of Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }}
            is experiencing {{ "{{" }} printf "%.0f" $value {{ "}}" }} failures with LIST/WATCH requests
            to the Kubernetes API in the last 5 minutes.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusKubernetesListWatchFailures.md
          summary: Requests in Kubernetes SD are failing.
        expr: |
          increase(prometheus_sd_kubernetes_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusNotificationQueueRunningFull
        annotations:
          description: Alert notification queue of Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }}
            is running full.
          summary: Prometheus alert notification queue predicted to run full in less
            than 30m.
        expr: |
          # Without min_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          (
            predict_linear(prometheus_notifications_queue_length{job=~"prometheus-k8s|prometheus-user-workload"}[5m], 60 * 30)
          >
            min_over_time(prometheus_notifications_queue_capacity{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          )
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
        annotations:
          description: '{{ "{{" }} printf "%.1f" $value {{ "}}" }}% of alerts sent by Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }}
            to Alertmanager {{ "{{" }}$labels.alertmanager{{ "}}" }} were affected by errors.'
          summary: More than 1% of alerts sent by Prometheus to a specific Alertmanager
            were affected by errors.
        expr: |
          (
            rate(prometheus_notifications_errors_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          /
            rate(prometheus_notifications_sent_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          )
          * 100
          > 1
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusNotConnectedToAlertmanagers
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} is not connected
            to any Alertmanagers.
          summary: Prometheus is not connected to any Alertmanagers.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max_over_time(prometheus_notifications_alertmanagers_discovered{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) < 1
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusTSDBReloadsFailing
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} has detected
            {{ "{{" }}$value | humanize{{ "}}" }} reload failures over the last 3h.
          summary: Prometheus has issues reloading blocks from disk.
        expr: |
          increase(prometheus_tsdb_reloads_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[3h]) > 0
        for: 4h
        labels:
          severity: warning
      - alert: PrometheusTSDBCompactionsFailing
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} has detected
            {{ "{{" }}$value | humanize{{ "}}" }} compaction failures over the last 3h.
          summary: Prometheus has issues compacting blocks.
        expr: |
          increase(prometheus_tsdb_compactions_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[3h]) > 0
        for: 4h
        labels:
          severity: warning
      - alert: PrometheusNotIngestingSamples
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} is not ingesting
            samples.
          summary: Prometheus is not ingesting samples.
        expr: |
          (
            sum without(type) (rate(prometheus_tsdb_head_samples_appended_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m])) <= 0
          and
            (
              sum without(scrape_job) (prometheus_target_metadata_cache_entries{job=~"prometheus-k8s|prometheus-user-workload"}) > 0
            or
              sum without(rule_group) (prometheus_rule_group_rules{job=~"prometheus-k8s|prometheus-user-workload"}) > 0
            )
          )
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusDuplicateTimestamps
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} is dropping
            {{ "{{" }} printf "%.4g" $value  {{ "}}" }} samples/s with different values but duplicated
            timestamp.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusDuplicateTimestamps.md
          summary: Prometheus is dropping samples with duplicate timestamps.
        expr: |
          rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 1h
        labels:
          severity: warning
      - alert: PrometheusOutOfOrderTimestamps
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} is dropping
            {{ "{{" }} printf "%.4g" $value  {{ "}}" }} samples/s with timestamps arriving out of
            order.
          summary: Prometheus drops samples with out-of-order timestamps.
        expr: |
          rate(prometheus_target_scrapes_sample_out_of_order_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 1h
        labels:
          severity: warning
      - alert: PrometheusRemoteStorageFailures
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} failed to
            send {{ "{{" }} printf "%.1f" $value {{ "}}" }}% of the samples to {{ "{{" }} $labels.remote_name{{ "}}" }}:{{ "{{" }}
            $labels.url {{ "}}" }}
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusRemoteStorageFailures.md
          summary: Prometheus fails to send samples to remote storage.
        expr: |
          (
            (rate(prometheus_remote_storage_failed_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
          /
            (
              (rate(prometheus_remote_storage_failed_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
            +
              (rate(prometheus_remote_storage_succeeded_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
            )
          )
          * 100
          > 1
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusRemoteWriteBehind
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} remote write
            is {{ "{{" }} printf "%.1f" $value {{ "}}" }}s behind for {{ "{{" }} $labels.remote_name{{ "}}" }}:{{ "{{" }}
            $labels.url {{ "}}" }}.
          summary: Prometheus remote write is behind.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          (
            max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          - ignoring(remote_name, url) group_right
            max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          )
          > 120
        for: 15m
        labels:
          severity: info
      - alert: PrometheusRemoteWriteDesiredShards
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} remote write
            desired shards calculation wants to run {{ "{{" }} $value {{ "}}" }} shards for queue
            {{ "{{" }} $labels.remote_name{{ "}}" }}:{{ "{{" }} $labels.url {{ "}}" }}, which is more than the max
            of {{ "{{" }} printf `prometheus_remote_storage_shards_max{instance="%s",job=~"prometheus-k8s|prometheus-user-workload"}`
            $labels.instance | query | first | value {{ "}}" }}.
          summary: Prometheus remote write desired shards calculation wants to run
            more than configured max shards.
        expr: |
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          (
            max_over_time(prometheus_remote_storage_shards_desired{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          >
            max_over_time(prometheus_remote_storage_shards_max{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
          )
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusRuleFailures
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} has failed
            to evaluate {{ "{{" }} printf "%.0f" $value {{ "}}" }} rules in the last 5m.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusRuleFailures.md
          summary: Prometheus is failing rule evaluations.
        expr: |
          increase(prometheus_rule_evaluation_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusMissingRuleEvaluations
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} has missed
            {{ "{{" }} printf "%.0f" $value {{ "}}" }} rule group evaluations in the last 5m.
          summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
        expr: |
          increase(prometheus_rule_group_iterations_missed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusTargetLimitHit
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} has dropped
            {{ "{{" }} printf "%.0f" $value {{ "}}" }} targets because the number of targets exceeded
            the configured target_limit.
          summary: Prometheus has dropped targets because some scrape configs have
            exceeded the targets limit.
        expr: |
          increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusLabelLimitHit
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} has dropped
            {{ "{{" }} printf "%.0f" $value {{ "}}" }} targets because some samples exceeded the configured
            label_limit, label_name_length_limit or label_value_length_limit.
          summary: Prometheus has dropped targets because some scrape configs have
            exceeded the labels limit.
        expr: |
          increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusScrapeBodySizeLimitHit
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} has failed
            {{ "{{" }} printf "%.0f" $value {{ "}}" }} scrapes in the last 5m because some targets
            exceeded the configured body_size_limit.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusScrapeBodySizeLimitHit.md
          summary: Prometheus has dropped some targets that exceeded body size limit.
        expr: |
          increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusScrapeSampleLimitHit
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} has failed
            {{ "{{" }} printf "%.0f" $value {{ "}}" }} scrapes in the last 5m because some targets
            exceeded the configured sample_limit.
          summary: Prometheus has failed scrapes that have exceeded the configured
            sample limit.
        expr: |
          increase(prometheus_target_scrapes_exceeded_sample_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusTargetSyncFailure
        annotations:
          description: '{{ "{{" }} printf "%.0f" $value {{ "}}" }} targets in Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }}
            have failed to sync because invalid configuration was supplied.'
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusTargetSyncFailure.md
          summary: Prometheus has failed to sync targets.
        expr: |
          increase(prometheus_target_sync_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[30m]) > 0
        for: 5m
        labels:
          severity: critical
      - alert: PrometheusHighQueryLoad
        annotations:
          description: Prometheus {{ "{{" }}$labels.namespace{{ "}}" }}/{{ "{{" }}$labels.pod{{ "}}" }} query API
            has less than 20% available capacity in its query engine for the last
            15 minutes.
          summary: Prometheus is reaching its maximum capacity serving concurrent
            requests.
        expr: |
          avg_over_time(prometheus_engine_queries{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) / max_over_time(prometheus_engine_queries_concurrent_max{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0.8
        for: 15m
        labels:
          severity: warning
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    labels:
      app.kubernetes.io/component: prometheus
      app.kubernetes.io/instance: k8s
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 3.2.1
      prometheus: k8s
      role: alert-rules
    name: prometheus-k8s-thanos-sidecar-rules
    namespace: cluster-monitoring
  spec:
    groups:
    - name: thanos-sidecar
      rules:
      - alert: ThanosSidecarBucketOperationsFailed
        annotations:
          description: Thanos Sidecar {{ "{{" }}$labels.instance{{ "}}" }} in {{ "{{" }}$labels.namespace{{ "}}" }}
            bucket operations are failing
          summary: Thanos Sidecar bucket operations are failing
        expr: |
          sum by (namespace, job, instance) (rate(thanos_objstore_bucket_operation_failures_total{job=~"prometheus-(k8s|user-workload)-thanos-sidecar"}[5m])) > 0
        for: 1h
        labels:
          severity: warning
      - alert: ThanosSidecarNoConnectionToStartedPrometheus
        annotations:
          description: Thanos Sidecar {{ "{{" }}$labels.instance{{ "}}" }} in {{ "{{" }}$labels.namespace{{ "}}" }}
            is unhealthy.
          summary: Thanos Sidecar cannot access Prometheus, even though Prometheus
            seems healthy and has reloaded WAL.
        expr: |
          thanos_sidecar_prometheus_up{job=~"prometheus-(k8s|user-workload)-thanos-sidecar"} == 0
          AND on (namespace, pod)
          prometheus_tsdb_data_replay_duration_seconds != 0
        for: 1h
        labels:
          severity: warning
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    labels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: prometheus-operator
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 0.81.0
      prometheus: k8s
      role: alert-rules
    name: prometheus-operator-rules
    namespace: cluster-monitoring
  spec:
    groups:
    - name: prometheus-operator
      rules:
      - alert: PrometheusOperatorListErrors
        annotations:
          description: Errors while performing List operations in controller {{ "{{" }}$labels.controller{{ "}}" }}
            in {{ "{{" }}$labels.namespace{{ "}}" }} namespace.
          summary: Errors while performing list operations in controller.
        expr: |
          (sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_failed_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[10m])) / sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[10m]))) > 0.4
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusOperatorWatchErrors
        annotations:
          description: Errors while performing watch operations in controller {{ "{{" }}$labels.controller{{ "}}" }}
            in {{ "{{" }}$labels.namespace{{ "}}" }} namespace.
          summary: Errors while performing watch operations in controller.
        expr: |
          (sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_failed_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m])) / sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) > 0.4
        for: 15m
        labels:
          severity: warning
      - alert: PrometheusOperatorSyncFailed
        annotations:
          description: Controller {{ "{{" }} $labels.controller {{ "}}" }} in {{ "{{" }} $labels.namespace
            {{ "}}" }} namespace fails to reconcile {{ "{{" }} $value {{ "}}" }} objects.
          summary: Last controller reconciliation failed
        expr: |
          min_over_time(prometheus_operator_syncs{status="failed",job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusOperatorReconcileErrors
        annotations:
          description: '{{ "{{" }} $value | humanizePercentage {{ "}}" }} of reconciling operations
            failed for {{ "{{" }} $labels.controller {{ "}}" }} controller in {{ "{{" }} $labels.namespace
            {{ "}}" }} namespace.'
          summary: Errors while reconciling objects.
        expr: |
          (sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) / (sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) > 0.1
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusOperatorStatusUpdateErrors
        annotations:
          description: '{{ "{{" }} $value | humanizePercentage {{ "}}" }} of status update operations
            failed for {{ "{{" }} $labels.controller {{ "}}" }} controller in {{ "{{" }} $labels.namespace
            {{ "}}" }} namespace.'
          summary: Errors while updating objects status.
        expr: |
          (sum by (cluster,controller,namespace) (rate(prometheus_operator_status_update_errors_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) / (sum by (cluster,controller,namespace) (rate(prometheus_operator_status_update_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) > 0.1
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusOperatorNodeLookupErrors
        annotations:
          description: Errors while reconciling Prometheus in {{ "{{" }} $labels.namespace
            {{ "}}" }} Namespace.
          summary: Errors while reconciling Prometheus.
        expr: |
          rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0.1
        for: 10m
        labels:
          severity: warning
      - alert: PrometheusOperatorNotReady
        annotations:
          description: Prometheus operator in {{ "{{" }} $labels.namespace {{ "}}" }} namespace isn't
            ready to reconcile {{ "{{" }} $labels.controller {{ "}}" }} resources.
          summary: Prometheus operator not ready
        expr: |
          min by (cluster,controller,namespace) (max_over_time(prometheus_operator_ready{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) == 0)
        for: 5m
        labels:
          severity: warning
      - alert: PrometheusOperatorRejectedResources
        annotations:
          description: Prometheus operator in {{ "{{" }} $labels.namespace {{ "}}" }} namespace rejected
            {{ "{{" }} printf "%0.0f" $value {{ "}}" }} {{ "{{" }} $labels.controller {{ "}}" }}/{{ "{{" }} $labels.resource
            {{ "}}" }} resources.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusOperatorRejectedResources.md
          summary: Resources rejected by Prometheus operator
        expr: |
          min_over_time(prometheus_operator_managed_resources{state="rejected",job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0
        for: 5m
        labels:
          severity: warning
    - name: config-reloaders
      rules:
      - alert: ConfigReloaderSidecarErrors
        annotations:
          description: |-
            Errors encountered while the {{ "{{" }}$labels.pod{{ "}}" }} config-reloader sidecar attempts to sync config in {{ "{{" }}$labels.namespace{{ "}}" }} namespace.
            As a result, configuration for service running in {{ "{{" }}$labels.pod{{ "}}" }} may be stale and cannot be updated anymore.
          summary: config-reloader sidecar has not had a successful reload for 10m
        expr: |
          max_over_time(reloader_last_reload_successful{namespace=~".+"}[5m]) == 0
        for: 10m
        labels:
          severity: warning
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    labels:
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/part-of: openshift-monitoring
    name: telemetry
    namespace: cluster-monitoring
  spec:
    groups:
    - name: telemeter.rules
      rules:
      - expr: max(federate_samples - federate_filtered_samples)
        record: cluster:telemetry_selected_series:count
      - alert: TelemeterClientFailures
        annotations:
          description: |-
            The telemeter client in namespace {{ "{{" }} $labels.namespace {{ "}}" }} fails {{ "{{" }} $value | humanize {{ "}}" }} of the requests to the telemeter service.
            Check the logs of the telemeter-client pod with the following command:
            oc logs -n openshift-monitoring deployment.apps/telemeter-client -c telemeter-client
            If the telemeter client fails to authenticate with the telemeter service, make sure that the global pull secret is up to date, see https://docs.openshift.com/container-platform/latest/openshift_images/managing_images/using-image-pull-secrets.html#images-update-global-pull-secret_using-image-pull-secrets for more details.
          runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/TelemeterClientFailures.md
          summary: Telemeter client fails to send metrics
        expr: |
          sum by (namespace) (
            rate(federate_requests_failed_total{job="telemeter-client"}[15m])
          ) /
          sum by (namespace) (
            rate(federate_requests_total{job="telemeter-client"}[15m])
          ) > 0.2
        for: 1h
        labels:
          severity: warning
- apiVersion: monitoring.coreos.com/v1
  kind: PrometheusRule
  metadata:
    labels:
      app.kubernetes.io/component: query-layer
      app.kubernetes.io/instance: thanos-querier
      app.kubernetes.io/managed-by: cluster-monitoring-operator
      app.kubernetes.io/name: thanos-query
      app.kubernetes.io/part-of: openshift-monitoring
      app.kubernetes.io/version: 0.37.2
    name: thanos-querier
    namespace: cluster-monitoring
  spec:
    groups:
    - name: thanos-query
      rules:
      - alert: ThanosQueryHttpRequestQueryErrorRateHigh
        annotations:
          description: Thanos Query {{ "{{" }}$labels.job{{ "}}" }} in {{ "{{" }}$labels.namespace{{ "}}" }} is failing
            to handle {{ "{{" }}$value | humanize{{ "}}" }}% of "query" requests.
          summary: Thanos Query is failing to handle requests.
        expr: |
          (
            sum by (namespace, job) (rate(http_requests_total{code=~"5..", job="thanos-querier", handler="query"}[5m]))
          /
            sum by (namespace, job) (rate(http_requests_total{job="thanos-querier", handler="query"}[5m]))
          ) * 100 > 5
        for: 1h
        labels:
          severity: warning
      - alert: ThanosQueryHttpRequestQueryRangeErrorRateHigh
        annotations:
          description: Thanos Query {{ "{{" }}$labels.job{{ "}}" }} in {{ "{{" }}$labels.namespace{{ "}}" }} is failing
            to handle {{ "{{" }}$value | humanize{{ "}}" }}% of "query_range" requests.
          summary: Thanos Query is failing to handle requests.
        expr: |
          (
            sum by (namespace, job) (rate(http_requests_total{code=~"5..", job="thanos-querier", handler="query_range"}[5m]))
          /
            sum by (namespace, job) (rate(http_requests_total{job="thanos-querier", handler="query_range"}[5m]))
          ) * 100 > 5
        for: 1h
        labels:
          severity: warning
      - alert: ThanosQueryGrpcServerErrorRate
        annotations:
          description: Thanos Query {{ "{{" }}$labels.job{{ "}}" }} in {{ "{{" }}$labels.namespace{{ "}}" }} is failing
            to handle {{ "{{" }}$value | humanize{{ "}}" }}% of requests.
          summary: Thanos Query is failing to handle requests.
        expr: |
          (
            sum by (namespace, job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job="thanos-querier"}[5m]))
          /
            sum by (namespace, job) (rate(grpc_server_started_total{job="thanos-querier"}[5m]))
          * 100 > 5
          )
        for: 1h
        labels:
          severity: warning
      - alert: ThanosQueryGrpcClientErrorRate
        annotations:
          description: Thanos Query {{ "{{" }}$labels.job{{ "}}" }} in {{ "{{" }}$labels.namespace{{ "}}" }} is failing
            to send {{ "{{" }}$value | humanize{{ "}}" }}% of requests.
          summary: Thanos Query is failing to send requests.
        expr: |
          (
            sum by (namespace, job) (rate(grpc_client_handled_total{grpc_code!="OK", job="thanos-querier"}[5m]))
          /
            sum by (namespace, job) (rate(grpc_client_started_total{job="thanos-querier"}[5m]))
          ) * 100 > 5
        for: 1h
        labels:
          severity: warning
      - alert: ThanosQueryHighDNSFailures
        annotations:
          description: Thanos Query {{ "{{" }}$labels.job{{ "}}" }} in {{ "{{" }}$labels.namespace{{ "}}" }} have
            {{ "{{" }}$value | humanize{{ "}}" }}% of failing DNS queries for store endpoints.
          summary: Thanos Query is having high number of DNS failures.
        expr: |
          (
            sum by (namespace, job) (rate(thanos_query_store_apis_dns_failures_total{job="thanos-querier"}[5m]))
          /
            sum by (namespace, job) (rate(thanos_query_store_apis_dns_lookups_total{job="thanos-querier"}[5m]))
          ) * 100 > 1
        for: 1h
        labels:
          severity: warning
      - alert: ThanosQueryOverload
        annotations:
          description: Thanos Query {{ "{{" }}$labels.job{{ "}}" }} in {{ "{{" }}$labels.namespace{{ "}}" }} has been
            overloaded for more than 15 minutes. This may be a symptom of excessive
            simultaneous complex requests, low performance of the Prometheus API,
            or failures within these components. Assess the health of the Thanos query
            instances, the connected Prometheus instances, look for potential senders
            of these requests and then contact support.
          summary: Thanos query reaches its maximum capacity serving concurrent requests.
        expr: |
          (
            max_over_time(thanos_query_concurrent_gate_queries_max[5m]) - avg_over_time(thanos_query_concurrent_gate_queries_in_flight[5m]) < 1
          )
        for: 1h
        labels:
          severity: warning
kind: List
metadata:
  resourceVersion: ""
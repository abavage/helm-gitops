---
# Source: cluster-monitoring/templates/namespace.cluster-monitoring.yaml
apiVersion: v1
kind: Namespace
metadata:
  annotations:
    openshift.io/display-name: cluster-monitoring
  labels:
    kubernetes.io/metadata.name: cluster-monitoring
  name: cluster-monitoring
---
# Source: cluster-monitoring/templates/serviceaccount.alertmanager-user-workload.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  annotations:
    #eks.amazonaws.com/role-arn: arn:aws:iam::281359555390:role/one-rosa-monitoring-sns-role
    eks.amazonaws.com/role-arn: arn:aws:iam::281359555390:role/one-rosa-monitoring-sns-role
  name: alertmanager-user-workload
  namespace: openshift-user-workload-monitoring
---
# Source: cluster-monitoring/templates/configmap.user-workload-monitoring-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    namespacesWithoutLabelEnforcement:
    - cluster-monitoring
    alertmanager:
      enabled: true
      enableAlertmanagerConfig: true
---
# Source: cluster-monitoring/templates/alertmanagerconfig.cluster-monitorimg.yaml
# you need to ensure the labels line up perfectly. 
# Since the AlertmanagerConfig automatically forces a namespace check, 
#the best approach is to hardcode the expected labels directly into your Rule.


apiVersion: monitoring.coreos.com/v1beta1
kind: AlertmanagerConfig
metadata:
  name: cluster-monitoring
  namespace: cluster-monitoring
spec:
  route:
    receiver: sns_receiver
    groupBy: 
      - alertname
      - severity
    routes:
      - receiver: "sns_receiver"
        matchers:
          - name: severity
            value: "info"
            matchType: "="
          - name: customer
            value: "true"
            matchType: "="
        groupWait: 10s
        groupInterval: 1m
        repeatInterval: 24h
      - receiver: "sns_receiver"
        matchers:
          - name: severity
            value: "warning"
            matchType: "="
          - name: customer
            value: "true"
            matchType: "="
        groupWait: 10s
        groupInterval: 1m
        repeatInterval: 3h
      - receiver: "sns_receiver"
        matchers:
          - name: severity
            value: "critical"
            matchType: "="
          - name: customer
            value: "true"
            matchType: "="
        groupWait: 10s
        groupInterval: 1m
        repeatInterval: 30m
  receivers:
  - name: sns_receiver
    snsConfigs:
      #- topicARN: arn:aws:sns:ap-southeast-2:281359555390:one-rosa-monitoring-sns-topic
      - topicARN: arn:aws:sns:ap-southeast-2:281359555390:one-rosa-monitoring-sns-topic
        sendResolved: false
        subject: 'Alert: {{ .GroupLabels.severity }} {{ .GroupLabels.alertname }}'
        sigv4:
          region: ap-southeast-2
---
# Source: cluster-monitoring/templates/prometheusrules/alertmanager-main-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  name: alertmanager-main-rules
  namespace: cluster-monitoring
spec:
  groups:
  - name: alertmanager.rules
    rules:
    - alert: AlertmanagerFailedReload
      annotations:
        description: Configuration has failed to load for {{ $labels.namespace }}/{{
          $labels.pod}}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedReload.md
        summary: Reloading an Alertmanager configuration has failed.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(alertmanager_config_last_reload_successful{job=~"alertmanager-main|alertmanager-user-workload"}[5m]) == 0
      for: 10m
      labels:
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
    - alert: AlertmanagerMembersInconsistent
      annotations:
        description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only
          found {{ $value }} members of the {{$labels.job}} cluster.
        summary: A member of an Alertmanager cluster has not found all other cluster
          members.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max_over_time(alertmanager_cluster_members{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
        < on (namespace,service) group_left
          count by (namespace,service) (max_over_time(alertmanager_cluster_members{job=~"alertmanager-main|alertmanager-user-workload"}[5m]))
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: AlertmanagerFailedToSendAlerts
      annotations:
        description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed
          to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration
          }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedToSendAlerts.md
        summary: An Alertmanager instance failed to send notifications.
      expr: |
        (
          rate(alertmanager_notifications_failed_total{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
        /
          ignoring (reason) group_left rate(alertmanager_notifications_total{job=~"alertmanager-main|alertmanager-user-workload"}[5m])
        )
        > 0.01
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: AlertmanagerClusterFailedToSendAlerts
      annotations:
        description: The minimum notification failure rate to {{ $labels.integration
          }} sent from any instance in the {{$labels.job}} cluster is {{ $value |
          humanizePercentage }}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerClusterFailedToSendAlerts.md
        summary: All Alertmanager instances in a cluster failed to send notifications
          to a critical integration.
      expr: |
        min by (namespace,service, integration) (
          rate(alertmanager_notifications_failed_total{job=~"alertmanager-main|alertmanager-user-workload", integration=~`.*`}[5m])
        /
          ignoring (reason) group_left rate(alertmanager_notifications_total{job=~"alertmanager-main|alertmanager-user-workload", integration=~`.*`}[5m])
        )
        > 0.01
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: AlertmanagerConfigInconsistent
      annotations:
        description: Alertmanager instances within the {{$labels.job}} cluster have
          different configurations.
        summary: Alertmanager instances within the same cluster have different configurations.
      expr: |
        count by (namespace,service) (
          count_values by (namespace,service) ("config_hash", alertmanager_config_hash{job=~"alertmanager-main|alertmanager-user-workload"})
        )
        != 1
      for: 20m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: AlertmanagerClusterDown
      annotations:
        description: '{{ $value | humanizePercentage }} of Alertmanager instances
          within the {{$labels.job}} cluster have been up for less than half of the
          last 5m.'
        summary: Half or more of the Alertmanager instances within the same cluster
          are down.
      expr: |
        (
          count by (namespace,service) (
            avg_over_time(up{job=~"alertmanager-main|alertmanager-user-workload"}[5m]) < 0.5
          )
        /
          count by (namespace,service) (
            up{job=~"alertmanager-main|alertmanager-user-workload"}
          )
        )
        >= 0.5
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/api-usage.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  name: api-usage
  namespace: cluster-monitoring
spec:
  groups:
  - name: pre-release-lifecycle
    rules:
    - alert: APIRemovedInNextReleaseInUse
      annotations:
        description: Deprecated API that will be removed in the next version is being
          used. Removing the workload that is using the {{ $labels.group }}.{{ $labels.version
          }}/{{ $labels.resource }} API might be necessary for a successful upgrade
          to the next cluster version. Refer to `oc get apirequestcounts {{ $labels.resource
          }}.{{ $labels.version }}.{{ $labels.group }} -o yaml` to identify the workload.
        summary: Deprecated API that will be removed in the next version is being
          used.
      expr: |
        group(apiserver_requested_deprecated_apis{removed_release="1.24"}) by (group,version,resource) and (sum by(group,version,resource) (rate(apiserver_request_total{system_client!="kube-controller-manager",system_client!="cluster-policy-controller"}[4h]))) > 0
      for: 1h
      labels:
        namespace: openshift-kube-apiserver
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: APIRemovedInNextEUSReleaseInUse
      annotations:
        description: Deprecated API that will be removed in the next EUS version is
          being used. Removing the workload that is using the {{ $labels.group }}.{{
          $labels.version }}/{{ $labels.resource }} API might be necessary for a successful
          upgrade to the next EUS cluster version. Refer to `oc get apirequestcounts
          {{ $labels.resource }}.{{ $labels.version }}.{{ $labels.group }} -o yaml`
          to identify the workload.
        summary: Deprecated API that will be removed in the next EUS version is being
          used.
      expr: |
        group(apiserver_requested_deprecated_apis{removed_release=~"1\\.2[45]"}) by (group,version,resource) and (sum by(group,version,resource) (rate(apiserver_request_total{system_client!="kube-controller-manager",system_client!="cluster-policy-controller"}[4h]))) > 0
      for: 1h
      labels:
        namespace: openshift-kube-apiserver
        severity: info
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/cluster-monitoring-operator-prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  name: cluster-monitoring-operator-prometheus-rules
  namespace: cluster-monitoring
spec:
  groups:
  - name: openshift-general.rules
    rules:
    - alert: TargetDown
      annotations:
        description: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service
          }} targets in {{ $labels.namespace }} namespace have been unreachable for
          more than 15 minutes. This may be a symptom of network connectivity issues,
          down nodes, or failures within these components. Assess the health of the
          infrastructure and nodes running these targets and then contact support.'
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/TargetDown.md
        summary: Some targets were not reachable from the monitoring server for an
          extended period of time.
      expr: |
        100 * ((
          1 - sum   by (job, namespace, service) (up and on(namespace, pod) kube_pod_info) /
              count by (job, namespace, service) (up and on(namespace, pod) kube_pod_info)
        ) or (
          count by (job, namespace, service) (up == 0) /
          count by (job, namespace, service) (up)
        )) > 10
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: openshift-kubernetes.rules
    rules:
    - expr: sum(rate(container_cpu_usage_seconds_total{container="",pod!=""}[5m]))
        BY (pod, namespace)
      record: pod:container_cpu_usage:sum
    - expr: sum(container_fs_usage_bytes{pod!=""}) BY (pod, namespace)
      record: pod:container_fs_usage_bytes:sum
    - expr: sum(container_memory_usage_bytes{container!=""}) BY (namespace)
      record: namespace:container_memory_usage_bytes:sum
    - expr: sum(rate(container_cpu_usage_seconds_total{container!="POD",container!=""}[5m]))
        BY (namespace)
      record: namespace:container_cpu_usage:sum
    - expr: sum(container_memory_usage_bytes{container="",pod!=""}) BY (cluster) /
        sum(machine_memory_bytes) BY (cluster)
      record: cluster:memory_usage:ratio
    - expr: sum(container_spec_cpu_shares{container="",pod!=""}) / 1000 / sum(machine_cpu_cores)
      record: cluster:container_spec_cpu_shares:ratio
    - expr: sum(rate(container_cpu_usage_seconds_total{container="",pod!=""}[5m]))
        / sum(machine_cpu_cores)
      record: cluster:container_cpu_usage:ratio
    - expr: |
        sum by(namespace,pod, interface) (irate(container_network_receive_bytes_total{pod!=""}[5m]))
        +
        on(namespace,pod, interface) group_left(network_name) topk by(namespace,pod, interface) (1, pod_network_name_info)
      record: pod_interface_network:container_network_receive_bytes:irate5m
    - expr: |
        sum by(namespace,pod, interface) (irate(container_network_transmit_bytes_total{pod!=""}[5m]))
        +
        on(namespace,pod, interface) group_left(network_name) topk by(namespace,pod, interface) (1, pod_network_name_info)
      record: pod_interface_network:container_network_transmit_bytes_total:irate5m
    - expr: max without(endpoint, instance, job, pod, service) (kube_node_labels and
        on(node) kube_node_role{role="master"})
      labels:
        label_node_role_kubernetes_io: master
        label_node_role_kubernetes_io_master: "true"
      record: cluster:master_nodes
    - expr: max without(endpoint, instance, job, pod, service) (kube_node_labels and
        on(node) kube_node_role{role="infra"})
      labels:
        label_node_role_kubernetes_io_infra: "true"
      record: cluster:infra_nodes
    - expr: max without(endpoint, instance, job, pod, service) (cluster:master_nodes
        and on(node) cluster:infra_nodes)
      labels:
        label_node_role_kubernetes_io_infra: "true"
        label_node_role_kubernetes_io_master: "true"
      record: cluster:master_infra_nodes
    - expr: cluster:master_infra_nodes or on (node) cluster:master_nodes or on (node)
        cluster:infra_nodes or on (node) max without(endpoint, instance, job, pod,
        service) (kube_node_labels)
      record: cluster:nodes_roles
    - expr: kube_node_labels and on(node) (sum(label_replace(node_cpu_info, "node",
        "$1", "instance", "(.*)")) by (node, package, core) == 2)
      labels:
        label_node_hyperthread_enabled: "true"
      record: cluster:hyperthread_enabled_nodes
    - expr: count(sum(virt_platform) by (instance, type, system_manufacturer, system_product_name,
        baseboard_manufacturer, baseboard_product_name)) by (type, system_manufacturer,
        system_product_name, baseboard_manufacturer, baseboard_product_name)
      record: cluster:virt_platform_nodes:sum
    - expr: |
        sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id) (
          (
            cluster:master_nodes
            * on(node) group_left() max by(node)
            (
              kube_node_status_capacity{resource="cpu",unit="core"}
            )
          )
          or on(node) (
            label_replace(cluster:infra_nodes, "label_node_role_kubernetes_io", "infra", "", "")
            * on(node) group_left() max by(node)
            (
              kube_node_status_capacity{resource="cpu",unit="core"}
            )
          )
          or on(node) (
            max without(endpoint, instance, job, pod, service)
            (
              kube_node_labels
            ) * on(node) group_left() max by(node)
            (
              kube_node_status_capacity{resource="cpu",unit="core"}
            )
          )
        )
      record: cluster:capacity_cpu_cores:sum
    - expr: |
        clamp_max(
          label_replace(
            sum by(instance, package, core) (
              node_cpu_info{core!="",package!=""}
              or
              # Assume core = cpu and package = 0 for platforms that don't expose core/package labels.
              label_replace(label_join(node_cpu_info{core="",package=""}, "core", "", "cpu"), "package", "0", "package", "")
            ) > 1,
            "label_node_hyperthread_enabled",
            "true",
            "instance",
            "(.*)"
          ) or on (instance, package)
          label_replace(
            sum by(instance, package, core) (
              label_replace(node_cpu_info{core!="",package!=""}
              or
              # Assume core = cpu and package = 0 for platforms that don't expose core/package labels.
              label_join(node_cpu_info{core="",package=""}, "core", "", "cpu"), "package", "0", "package", "")
            ) <= 1,
            "label_node_hyperthread_enabled",
            "false",
            "instance",
            "(.*)"
          ),
          1
        )
      record: cluster:cpu_core_hyperthreading
    - expr: |
        topk by(node) (1, cluster:nodes_roles) * on (node)
          group_right( label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_node_openshift_io_os_id, label_kubernetes_io_arch,
                       label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra)
        label_replace( cluster:cpu_core_hyperthreading, "node", "$1", "instance", "(.*)" )
      record: cluster:cpu_core_node_labels
    - expr: count(cluster:cpu_core_node_labels) by (label_beta_kubernetes_io_instance_type,
        label_node_hyperthread_enabled)
      record: cluster:capacity_cpu_cores_hyperthread_enabled:sum
    - expr: |
        sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io)
        (
          (
            cluster:master_nodes
            * on(node) group_left() max by(node)
            (
              kube_node_status_capacity{resource="memory",unit="byte"}
            )
          )
          or on(node)
          (
            max without(endpoint, instance, job, pod, service)
            (
              kube_node_labels
            )
            * on(node) group_left() max by(node)
            (
              kube_node_status_capacity{resource="memory",unit="byte"}
            )
          )
        )
      record: cluster:capacity_memory_bytes:sum
    - expr: sum(1 - rate(node_cpu_seconds_total{mode="idle"}[2m]) * on(namespace,
        pod) group_left(node) node_namespace_pod:kube_pod_info:{pod=~"node-exporter.+"})
      record: cluster:cpu_usage_cores:sum
    - expr: sum(node_memory_MemTotal_bytes{job="node-exporter"} - node_memory_MemAvailable_bytes{job="node-exporter"})
      record: cluster:memory_usage_bytes:sum
    - expr: sum(rate(container_cpu_usage_seconds_total{namespace!~"openshift-.+",pod!="",container=""}[5m]))
      record: workload:cpu_usage_cores:sum
    - expr: cluster:cpu_usage_cores:sum - workload:cpu_usage_cores:sum
      record: openshift:cpu_usage_cores:sum
    - expr: sum(container_memory_working_set_bytes{namespace!~"openshift-.+",pod!="",container=""})
      record: workload:memory_usage_bytes:sum
    - expr: cluster:memory_usage_bytes:sum - workload:memory_usage_bytes:sum
      record: openshift:memory_usage_bytes:sum
    - expr: sum(cluster:master_nodes or on(node) kube_node_labels ) BY (label_beta_kubernetes_io_instance_type,
        label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id)
      record: cluster:node_instance_type_count:sum
    - expr: |
        sum by(provisioner) (
          topk by (namespace, persistentvolumeclaim) (
            1, kube_persistentvolumeclaim_resource_requests_storage_bytes
          ) * on(namespace, persistentvolumeclaim) group_right()
          topk by(namespace, persistentvolumeclaim) (
            1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))
          )
        )
      record: cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum
    - expr: (sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master="",label_node_role_kubernetes_io_infra=""}
        or absent(__does_not_exist__)*0)) + ((sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master="true"}
        or absent(__does_not_exist__)*0) * ((max(cluster_master_schedulable == 1)*0+1)
        or (absent(cluster_master_schedulable == 1)*0))))
      record: workload:capacity_physical_cpu_cores:sum
    - expr: min_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])
      record: cluster:usage:workload:capacity_physical_cpu_cores:min:5m
    - expr: max_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])
      record: cluster:usage:workload:capacity_physical_cpu_cores:max:5m
    - expr: |
        sum  by (provisioner) (
          topk by (namespace, persistentvolumeclaim) (
            1, kubelet_volume_stats_used_bytes
          ) * on (namespace,persistentvolumeclaim) group_right()
          topk by (namespace, persistentvolumeclaim) (
            1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))
          )
        )
      record: cluster:kubelet_volume_stats_used_bytes:provisioner:sum
    - expr: sum by (instance) (apiserver_storage_objects != -1)
      record: instance:etcd_object_counts:sum
    - expr: topk(500, max by(resource) (apiserver_storage_objects != -1))
      record: cluster:usage:resources:sum
    - expr: count(count (kube_pod_restart_policy{type!="Always",namespace!~"openshift-.+"})
        by (namespace,pod))
      record: cluster:usage:pods:terminal:workload:sum
    - expr: sum(max(kubelet_containers_per_pod_count_sum) by (instance))
      record: cluster:usage:containers:sum
    - expr: count(cluster:cpu_core_node_labels) by (label_kubernetes_io_arch, label_node_hyperthread_enabled,
        label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
      record: node_role_os_version_machine:cpu_capacity_cores:sum
    - expr: count(max(cluster:cpu_core_node_labels) by (node, package, label_beta_kubernetes_io_instance_type,
        label_node_hyperthread_enabled, label_node_role_kubernetes_io) ) by ( label_beta_kubernetes_io_instance_type,
        label_node_hyperthread_enabled, label_node_role_kubernetes_io)
      record: cluster:capacity_cpu_sockets_hyperthread_enabled:sum
    - expr: count (max(cluster:cpu_core_node_labels) by (node, package, label_kubernetes_io_arch,
        label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
        ) by (label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)
      record: node_role_os_version_machine:cpu_capacity_sockets:sum
    - expr: max(alertmanager_integrations{namespace="openshift-monitoring"})
      record: cluster:alertmanager_integrations:max
    - expr: sum by(plugin_name, volume_mode)(pv_collector_total_pv_count{volume_plugin!~".*-e2e-.*"})
      record: cluster:kube_persistentvolume_plugin_type_counts:sum
    - expr: |
        sum(
          min by (node) (kube_node_status_condition{condition="Ready",status="true"})
            and
          max by (node) (kube_node_role{role="master"})
        ) == bool sum(kube_node_role{role="master"})
      record: cluster:control_plane:all_nodes_ready
    - expr: max by (profile) (cluster_monitoring_operator_collection_profile == 1)
      record: profile:cluster_monitoring_operator_collection_profile:max
    - alert: ClusterMonitoringOperatorReconciliationErrors
      annotations:
        description: Errors are occurring during reconciliation cycles. Inspect the
          cluster-monitoring-operator log for potential root causes.
        summary: Cluster Monitoring Operator is experiencing unexpected reconciliation
          errors.
      expr: max_over_time(cluster_monitoring_operator_last_reconciliation_successful[5m])
        == 0
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ClusterMonitoringOperatorDeprecatedConfig
      annotations:
        description: The configuration field {{ $labels.field }} in {{ $labels.configmap
          }} was deprecated in {{ $labels.deprecation_version }} and has no effect.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/ClusterMonitoringOperatorDeprecatedConfig.md
        summary: Cluster Monitoring Operator is being used with deprecated configuration.
      expr: max by (configmap, field, deprecation_version) (cluster_monitoring_operator_deprecated_config_in_use)
        == 1
      for: 1h
      labels:
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: AlertmanagerReceiversNotConfigured
      annotations:
        description: Alerts are not configured to be sent to a notification system,
          meaning that you may not be notified in a timely fashion when important
          failures occur. Check the OpenShift documentation to learn how to configure
          notifications with Alertmanager.
        summary: Receivers (notification integrations) are not configured on Alertmanager
      expr: cluster:alertmanager_integrations:max == 0
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeDeploymentReplicasMismatch
      annotations:
        description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has
          not matched the expected number of replicas for longer than 15 minutes.
          This indicates that cluster infrastructure is unable to start or restart
          the necessary components. This most often occurs when one or more nodes
          are down or partioned from the cluster, or a fault occurs on the node that
          prevents the workload from starting. In rare cases this may indicate a new
          version of a cluster component cannot start due to a bug or configuration
          error. Assess the pods for this deployment to verify they are running on
          healthy nodes and then contact support.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeDeploymentReplicasMismatch.md
        summary: Deployment has not matched the expected number of replicas
      expr: |
        (((
          kube_deployment_spec_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            >
          kube_deployment_status_replicas_available{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
        ) and (
          changes(kube_deployment_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
            ==
          0
        )) * on() group_left cluster:control_plane:all_nodes_ready) > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - expr: avg_over_time((((count((max by (node) (up{job="kubelet",metrics_path="/metrics"}
        == 1) and max by (node) (kube_node_status_condition{condition="Ready",status="true"}
        == 1) and min by (node) (kube_node_spec_unschedulable == 0))) / scalar(count(min
        by (node) (kube_node_spec_unschedulable == 0))))))[5m:1s])
      record: cluster:usage:kube_schedulable_node_ready_reachable:avg5m
    - expr: avg_over_time((count(max by (node) (kube_node_status_condition{condition="Ready",status="true"}
        == 1)) / scalar(count(max by (node) (kube_node_status_condition{condition="Ready",status="true"}))))[5m:1s])
      record: cluster:usage:kube_node_ready:avg5m
    - expr: (max without (condition,container,endpoint,instance,job,service) (((kube_pod_status_ready{condition="false"}
        == 1)*0 or (kube_pod_status_ready{condition="true"} == 1)) * on(pod,namespace)
        group_left() group by (pod,namespace) (kube_pod_status_phase{phase=~"Running|Unknown|Pending"}
        == 1)))
      record: kube_running_pod_ready
    - expr: avg(kube_running_pod_ready{namespace=~"openshift-.*"})
      record: cluster:usage:openshift:kube_running_pod_ready:avg
    - expr: avg(kube_running_pod_ready{namespace!~"openshift-.*"})
      record: cluster:usage:workload:kube_running_pod_ready:avg
    - alert: KubePodNotScheduled
      annotations:
        description: |-
          Pod {{ $labels.namespace }}/{{ $labels.pod }} cannot be scheduled for more than 30 minutes.
          Check the details of the pod with the following command:
          oc describe -n {{ $labels.namespace }} pod {{ $labels.pod }}
        summary: Pod cannot be scheduled.
      expr: last_over_time(kube_pod_status_unschedulable{namespace=~"(openshift-.*|kube-.*|default)"}[5m])
        == 1
      for: 30m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - interval: 30s
    name: kubernetes-recurring.rules
    rules:
    - expr: sum_over_time(workload:capacity_physical_cpu_cores:sum[30s:1s]) + ((cluster:usage:workload:capacity_physical_cpu_core_seconds
        offset 25s) or (absent(cluster:usage:workload:capacity_physical_cpu_core_seconds
        offset 25s)*0))
      record: cluster:usage:workload:capacity_physical_cpu_core_seconds
  - name: openshift-ingress.rules
    rules:
    - expr: sum by (code) (rate(haproxy_server_http_responses_total[5m]) > 0)
      record: code:cluster:ingress_http_request_count:rate5m:sum
    - expr: sum (rate(haproxy_frontend_bytes_in_total[5m]))
      record: cluster:usage:ingress_frontend_bytes_in:rate5m:sum
    - expr: sum (rate(haproxy_frontend_bytes_out_total[5m]))
      record: cluster:usage:ingress_frontend_bytes_out:rate5m:sum
    - expr: sum (haproxy_frontend_current_sessions)
      record: cluster:usage:ingress_frontend_connections:sum
    - expr: sum(max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{code!~"2xx|1xx|4xx|3xx",exported_namespace!~"openshift-.*"}[5m])
        > 0)) / sum (max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{exported_namespace!~"openshift-.*"}[5m])))
        or absent(__does_not_exist__)*0
      record: cluster:usage:workload:ingress_request_error:fraction5m
    - expr: sum (max without(service,endpoint,container,pod,job,namespace) (irate(haproxy_server_http_responses_total{exported_namespace!~"openshift-.*"}[5m])))
        or absent(__does_not_exist__)*0
      record: cluster:usage:workload:ingress_request_total:irate5m
    - expr: sum(max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{code!~"2xx|1xx|4xx|3xx",exported_namespace=~"openshift-.*"}[5m])
        > 0)) / sum (max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{exported_namespace=~"openshift-.*"}[5m])))
        or absent(__does_not_exist__)*0
      record: cluster:usage:openshift:ingress_request_error:fraction5m
    - expr: sum (max without(service,endpoint,container,pod,job,namespace) (irate(haproxy_server_http_responses_total{exported_namespace=~"openshift-.*"}[5m])))
        or absent(__does_not_exist__)*0
      record: cluster:usage:openshift:ingress_request_total:irate5m
    - expr: sum(ingress_controller_aws_nlb_active) or vector(0)
      record: cluster:ingress_controller_aws_nlb_active:sum
  - name: openshift-build.rules
    rules:
    - expr: sum by (strategy) (openshift_build_status_phase_total)
      record: openshift:build_by_strategy:sum
  - name: openshift-monitoring.rules
    rules:
    - expr: sum by (job,namespace) (max without(instance) (prometheus_tsdb_head_series{namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}))
      record: openshift:prometheus_tsdb_head_series:sum
    - expr: sum by(job,namespace) (max without(instance) (rate(prometheus_tsdb_head_samples_appended_total{namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[2m])))
      record: openshift:prometheus_tsdb_head_samples_appended_total:sum
    - expr: sum by (namespace) (max without(instance) (container_memory_working_set_bytes{namespace=~"openshift-monitoring|openshift-user-workload-monitoring",
        container=""}))
      record: monitoring:container_memory_working_set_bytes:sum
    - expr: topk(3, sum by(namespace, job)(sum_over_time(scrape_series_added[1h])))
      record: namespace_job:scrape_series_added:topk3_sum1h
    - expr: topk(3, max by(namespace, job) (topk by(namespace,job) (1, scrape_samples_post_metric_relabeling)))
      record: namespace_job:scrape_samples_post_metric_relabeling:topk3
    - expr: sum by(exported_service) (rate(haproxy_server_http_responses_total{exported_namespace="openshift-monitoring",
        exported_service=~"alertmanager-main|prometheus-k8s"}[5m]))
      record: monitoring:haproxy_server_http_responses_total:sum
    - expr: max by (cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job="kube-state-metrics",
        owner_kind="ReplicationController"},"replicationcontroller", "$1", "owner_name",
        "(.*)") * on(replicationcontroller, namespace) group_left(owner_name) topk
        by(replicationcontroller, namespace) (1, max by (replicationcontroller, namespace,
        owner_name) (kube_replicationcontroller_owner{job="kube-state-metrics"})),"workload",
        "$1", "owner_name", "(.*)"))
      labels:
        workload_type: deploymentconfig
      record: namespace_workload_pod:kube_pod_owner:relabel
  - name: openshift-etcd-telemetry.rules
    rules:
    - expr: sum by (instance) (etcd_mvcc_db_total_size_in_bytes{job="etcd"})
      record: instance:etcd_mvcc_db_total_size_in_bytes:sum
    - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_wal_fsync_duration_seconds_bucket{job="etcd"}[5m])))
      labels:
        quantile: "0.99"
      record: instance:etcd_disk_wal_fsync_duration_seconds:histogram_quantile
    - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_network_peer_round_trip_time_seconds_bucket{job="etcd"}[5m])))
      labels:
        quantile: "0.99"
      record: instance:etcd_network_peer_round_trip_time_seconds:histogram_quantile
    - expr: sum by (instance) (etcd_mvcc_db_total_size_in_use_in_bytes{job="etcd"})
      record: instance:etcd_mvcc_db_total_size_in_use_in_bytes:sum
    - expr: histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_backend_commit_duration_seconds_bucket{job="etcd"}[5m])))
      labels:
        quantile: "0.99"
      record: instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile
  - name: openshift-sre.rules
    rules:
    - expr: sum(rate(apiserver_request_total{job="apiserver"}[10m])) BY (code)
      record: code:apiserver_request_total:rate:sum
  - name: apiserver-list-watch.rules
    rules:
    - expr: sum by(verb) (rate(apiserver_request_total{verb=~"LIST|WATCH",code=~"2.."}[5m]))
      record: apiserver_list_watch_request_success_total:rate:sum
  - name: general.rules
    rules:
    - alert: Watchdog
      annotations:
        description: |
          This is an alert meant to ensure that the entire alerting pipeline is functional.
          This alert is always firing, therefore it should always be firing in Alertmanager
          and always fire against a receiver. There are integrations with various notification
          mechanisms that send a notification when this alert is not firing. For example the
          "DeadMansSnitch" integration in PagerDuty.
        summary: An alert that should always be firing to certify that Alertmanager
          is working properly.
      expr: vector(1)
      labels:
        severity: info
        customer: "true"
        namespace: cluster-monitoring
  - name: node-network
    rules:
    - alert: NodeNetworkInterfaceFlapping
      annotations:
        description: Network interface "{{ $labels.device }}" changing its up status
          often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}
        summary: Network interface is often changing its status
      expr: |
        changes(node_network_up{job="node-exporter",device!~"veth.+|tunbr"}[2m]) > 2
      for: 2m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: kube-prometheus-node-recording.rules
    rules:
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[3m]))
        BY (instance)
      record: instance:node_cpu:rate:sum
    - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
      record: instance:node_network_receive_bytes:rate:sum
    - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
      record: instance:node_network_transmit_bytes:rate:sum
    - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m]))
      record: cluster:node_cpu:sum_rate5m
    - expr: cluster:node_cpu:sum_rate5m / count(sum(node_cpu_seconds_total) BY (instance,
        cpu))
      record: cluster:node_cpu:ratio
  - name: kube-prometheus-general.rules
    rules:
    - expr: count without(instance, pod, node) (up == 1)
      record: count:up1
    - expr: count without(instance, pod, node) (up == 0)
      record: count:up0
---
# Source: cluster-monitoring/templates/prometheusrules/dns.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  name: dns
  namespace: cluster-monitoring
spec:
  groups:
  - name: openshift-dns.rules
    rules:
    - alert: CoreDNSPanicking
      annotations:
        description: '{{ $value }} CoreDNS panics observed on {{ $labels.instance
          }}'
        summary: CoreDNS panic
      expr: increase(coredns_panics_total[10m]) > 0
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: CoreDNSHealthCheckSlow
      annotations:
        description: CoreDNS Health Checks are slowing down (instance {{ $labels.instance
          }})
        summary: CoreDNS health checks
      expr: histogram_quantile(.95, sum(rate(coredns_health_request_duration_seconds_bucket[5m]))
        by (instance, le)) > 10
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: CoreDNSErrorsHigh
      annotations:
        description: CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage
          }} of requests.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-dns-operator/CoreDNSErrorsHigh.md
        summary: CoreDNS serverfail
      expr: |
        (sum by(namespace) (rate(coredns_dns_responses_total{rcode="SERVFAIL"}[5m]))
          /
        sum by(namespace) (rate(coredns_dns_responses_total[5m])))
        > 0.01
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/image-registry-operator-alerts.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  name: image-registry-operator-alerts
  namespace: cluster-monitoring
spec:
  groups:
  - name: pvc-problem-detector.rules
    rules:
    - alert: ImageRegistryStorageReadOnly
      annotations:
        description: The image registry storage is read-only. Read-only storage affects
          direct pushes to the image registry, and pull-through proxy caching. In
          the case of pull-through proxy caching, read-only storage is particularly
          important because without it the image registry won't be actually caching
          anything. Please verify your backing storage solution and make sure the
          volume mounted on the image-registry pods is writable to avoid potential
          outages.
        message: The image registry storage is read-only and no images will be committed
          to storage.
        summary: The image registry storage is read-only and no images will be committed
          to storage.
      expr: sum without(instance, pod, operation) (rate(imageregistry_storage_errors_total{code="READ_ONLY_FILESYSTEM"}[5m]))
        > 0
      for: 10m
      labels:
        kubernetes_operator_part_of: image-registry
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ImageRegistryStorageFull
      annotations:
        description: The image registry storage disk is full. A full disk affects
          direct pushes to the image registry, and pull-through proxy caching. In
          the case of pull-through proxy caching, disk space is particularly important
          because without it the image registry won't be actually caching anything.
          Please verify your backing storage solution and make sure the volume mounted
          on the image-registry pods have enough free disk space to avoid potential
          outages.
        message: The image registry storage disk is full and no images will be committed
          to storage.
        summary: The image registry storage disk is full and no images will be committed
          to storage.
      expr: sum without(instance, pod, operation) (rate(imageregistry_storage_errors_total{code="DEVICE_OUT_OF_SPACE"}[5m]))
        > 0
      for: 10m
      labels:
        kubernetes_operator_part_of: image-registry
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/image-registry-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  name: image-registry-rules
  namespace: cluster-monitoring
spec:
  groups:
  - name: imageregistry.operations.rules
    rules:
    - expr: |
        label_replace(
          label_replace(
            sum by (operation) (imageregistry_request_duration_seconds_count{operation="BlobStore.ServeBlob"}), "operation", "get", "operation", "(.+)"
          ), "resource_type", "blob", "resource_type", ""
        )
      record: imageregistry:operations_count:sum
    - expr: |
        label_replace(
          label_replace(
            sum by (operation) (imageregistry_request_duration_seconds_count{operation="BlobStore.Create"}), "operation", "create", "operation", "(.+)"
          ), "resource_type", "blob", "resource_type", ""
        )
      record: imageregistry:operations_count:sum
    - expr: |
        label_replace(
          label_replace(
            sum by (operation) (imageregistry_request_duration_seconds_count{operation="ManifestService.Get"}), "operation", "get", "operation", "(.+)"
          ), "resource_type", "manifest", "resource_type", ""
        )
      record: imageregistry:operations_count:sum
    - expr: |
        label_replace(
          label_replace(
            sum by (operation) (imageregistry_request_duration_seconds_count{operation="ManifestService.Put"}), "operation", "create", "operation", "(.+)"
          ), "resource_type", "manifest", "resource_type", ""
        )
      record: imageregistry:operations_count:sum
---
# Source: cluster-monitoring/templates/prometheusrules/imagestreams-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: imagestreams-rules
spec:
  groups:
  - name: imagestreams.rules
    rules:
    - expr: sum by (location, source) (image_registry_image_stream_tags_total)
      record: imageregistry:imagestreamtags_count:sum
---
# Source: cluster-monitoring/templates/prometheusrules/ingress-operator.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  name: ingress-operator
  namespace: cluster-monitoring
spec:
  groups:
  - name: openshift-ingress.rules
    rules:
    - alert: HAProxyReloadFail
      annotations:
        description: This alert fires when HAProxy fails to reload its configuration,
          which will result in the router not picking up recently created or modified
          routes.
        message: HAProxy reloads are failing on {{ $labels.pod }}. Router is not respecting
          recently created or modified routes
        summary: HAProxy reload failure
      expr: template_router_reload_failure == 1
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: HAProxyDown
      annotations:
        description: This alert fires when metrics report that HAProxy is down.
        message: HAProxy metrics are reporting that HAProxy is down on pod {{ $labels.namespace
          }} / {{ $labels.pod }}
        summary: HAProxy is down
      expr: haproxy_up == 0
      for: 5m
      labels:
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
    - alert: IngressControllerDegraded
      annotations:
        description: This alert fires when the IngressController status is degraded.
        message: |
          The {{ $labels.namespace }}/{{ $labels.name }} ingresscontroller is
          degraded: {{ $labels.reason }}.
        summary: IngressController is degraded
      expr: ingress_controller_conditions{condition="Degraded"} == 1
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: IngressControllerUnavailable
      annotations:
        description: This alert fires when the IngressController is not available.
        message: |
          The {{ $labels.namespace }}/{{ $labels.name }} ingresscontroller is
          unavailable: {{ $labels.reason }}.
        summary: IngressController is unavailable
      expr: ingress_controller_conditions{condition="Available"} == 0
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - expr: min(route_metrics_controller_routes_per_shard)
      record: cluster:route_metrics_controller_routes_per_shard:min
    - expr: max(route_metrics_controller_routes_per_shard)
      record: cluster:route_metrics_controller_routes_per_shard:max
    - expr: avg(route_metrics_controller_routes_per_shard)
      record: cluster:route_metrics_controller_routes_per_shard:avg
    - expr: quantile(0.5, route_metrics_controller_routes_per_shard)
      record: cluster:route_metrics_controller_routes_per_shard:median
    - expr: sum (openshift_route_info) by (tls_termination)
      record: cluster:openshift_route_info:tls_termination:sum
  - name: openshift-ingress-to-route-controller.rules
    rules:
    - alert: IngressWithoutClassName
      annotations:
        description: This alert fires when there is an Ingress with an unset IngressClassName
          for longer than one day.
        message: Ingress {{ $labels.namespace }}/{{ $labels.name }} is missing the
          IngressClassName for 1 day.
        summary: Ingress without IngressClassName for 1 day
      expr: openshift_ingress_to_route_controller_ingress_without_class_name == 1
      for: 1d
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: UnmanagedRoutes
      annotations:
        description: This alert fires when there is a Route owned by an unmanaged
          Ingress.
        message: Route {{ $labels.namespace }}/{{ $labels.name }} is owned by an unmanaged
          Ingress.
        summary: Route owned by an Ingress no longer managed
      expr: openshift_ingress_to_route_controller_route_with_unmanaged_owner == 1
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/insights-prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  name: insights-prometheus-rules
  namespace: cluster-monitoring
spec:
  groups:
  - name: insights
    rules:
    - alert: InsightsDisabled
      annotations:
        description: 'Insights operator is disabled. In order to enable Insights and
          benefit from recommendations specific to your cluster, please follow steps
          listed in the documentation: https://docs.openshift.com/container-platform/latest/support/remote_health_monitoring/enabling-remote-health-reporting.html'
        summary: Insights operator is disabled.
      expr: max without (job, pod, service, instance) (cluster_operator_conditions{name="insights",
        condition="Disabled"} == 1)
      for: 5m
      labels:
        namespace: openshift-insights
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: SimpleContentAccessNotAvailable
      annotations:
        description: Simple content access (SCA) is not enabled. Once enabled, Insights
          Operator can automatically import the SCA certificates from Red Hat OpenShift
          Cluster Manager making it easier to use the content provided by your Red
          Hat subscriptions when creating container images. See https://docs.openshift.com/container-platform/latest/cicd/builds/running-entitled-builds.html
          for more information.
        summary: Simple content access certificates are not available.
      expr: ' max without (job, pod, service, instance) (max_over_time(cluster_operator_conditions{name="insights",
        condition="SCAAvailable", reason="NotFound"}[5m]) == 0)'
      for: 5m
      labels:
        #namespace: openshift-insights
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: InsightsRecommendationActive
      annotations:
        description: Insights recommendation "{{ $labels.description }}" with total
          risk "{{ $labels.total_risk }}" was detected on the cluster. More information
          is available at {{ $labels.info_link }}.
        summary: An Insights recommendation is active for this cluster.
      expr: insights_recommendation_active == 1
      for: 5m
      labels:
        severity: info
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/kube-state-metrics-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: kube-state-metrics-rules
spec:
  groups:
  - name: kube-state-metrics
    rules:
    - alert: KubeStateMetricsListErrors
      annotations:
        description: kube-state-metrics is experiencing errors at an elevated rate
          in list operations. This is likely causing it to not be able to expose metrics
          about Kubernetes objects correctly or at all.
        summary: kube-state-metrics is experiencing errors in list operations.
      expr: |
        (sum(rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
          /
        sum(rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])) by (cluster))
        > 0.01
      for: 15m
      labels:
        #namespace: openshift-monitoring
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeStateMetricsWatchErrors
      annotations:
        description: kube-state-metrics is experiencing errors at an elevated rate
          in watch operations. This is likely causing it to not be able to expose
          metrics about Kubernetes objects correctly or at all.
        summary: kube-state-metrics is experiencing errors in watch operations.
      expr: |
        (sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
          /
        sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])) by (cluster))
        > 0.01
      for: 15m
      labels:
        namespace: openshift-monitoring
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/kubernetes-monitoring-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: kubernetes-monitoring-rules
spec:
  groups:
  - name: kubernetes-apps
    rules:
    - alert: KubePodCrashLooping
      annotations:
        description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container
          }}) is in waiting state (reason: "CrashLoopBackOff").'
        summary: Pod is crash looping.
      expr: |
        max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff", namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m]) >= 1
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubePodNotReady
      annotations:
        description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready
          state for longer than 15 minutes.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePodNotReady.md
        summary: Pod has been in a non-ready state for more than 15 minutes.
      expr: |
        sum by (namespace, pod, cluster) (
          max by(namespace, pod, cluster) (
            kube_pod_status_phase{namespace=~"(openshift-.*|kube-.*|default)", job="kube-state-metrics", phase=~"Pending|Unknown"}
            unless ignoring(phase) (kube_pod_status_unschedulable{job="kube-state-metrics"} == 1)
          ) * on(namespace, pod, cluster) group_left(owner_kind) topk by(namespace, pod, cluster) (
            1, max by(namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"})
          )
        ) > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeDeploymentGenerationMismatch
      annotations:
        description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment
          }} does not match, this indicates that the Deployment has failed but has
          not been rolled back.
        summary: Deployment generation mismatch due to possible roll-back
      expr: |
        kube_deployment_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          !=
        kube_deployment_metadata_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeDeploymentRolloutStuck
      annotations:
        description: Rollout of deployment {{ $labels.namespace }}/{{ $labels.deployment
          }} is not progressing for longer than 15 minutes.
        summary: Deployment rollout is not progressing.
      expr: |
        kube_deployment_status_condition{condition="Progressing", status="false",namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
        != 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeStatefulSetReplicasMismatch
      annotations:
        description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }}
          has not matched the expected number of replicas for longer than 15 minutes.
        summary: StatefulSet has not matched the expected number of replicas.
      expr: |
        (
          kube_statefulset_status_replicas_ready{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
            !=
          kube_statefulset_status_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
        ) and (
          changes(kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[10m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeStatefulSetGenerationMismatch
      annotations:
        description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset
          }} does not match, this indicates that the StatefulSet has failed but has
          not been rolled back.
        summary: StatefulSet generation mismatch due to possible roll-back
      expr: |
        kube_statefulset_status_observed_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          !=
        kube_statefulset_metadata_generation{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeStatefulSetUpdateNotRolledOut
      annotations:
        description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }}
          update has not been rolled out.
        summary: StatefulSet update has not been rolled out.
      expr: |
        (
          max without (revision) (
            kube_statefulset_status_current_revision{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              unless
            kube_statefulset_status_update_revision{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          )
            *
          (
            kube_statefulset_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
              !=
            kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          )
        )  and (
          changes(kube_statefulset_status_replicas_updated{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
            ==
          0
        )
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeDaemonSetRolloutStuck
      annotations:
        description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has
          not finished or progressed for at least 30 minutes.
        summary: DaemonSet rollout is stuck.
      expr: |
        (
          (
            kube_daemonset_status_current_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
             !=
            kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          ) or (
            kube_daemonset_status_number_misscheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
             !=
            0
          ) or (
            kube_daemonset_status_updated_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
             !=
            kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          ) or (
            kube_daemonset_status_number_available{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
             !=
            kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          )
        ) and (
          changes(kube_daemonset_status_updated_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[5m])
            ==
          0
        )
      for: 30m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeContainerWaiting
      annotations:
        description: pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on
          container {{ $labels.container}} has been in waiting state for longer than
          1 hour.
        summary: Pod container waiting longer than 1 hour
      expr: |
        sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}) > 0
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeDaemonSetNotScheduled
      annotations:
        description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
          }} are not scheduled.'
        summary: DaemonSet pods are not scheduled.
      expr: |
        kube_daemonset_status_desired_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          -
        kube_daemonset_status_current_number_scheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeDaemonSetMisScheduled
      annotations:
        description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset
          }} are running where they are not supposed to run.'
        summary: DaemonSet pods are misscheduled.
      expr: |
        kube_daemonset_status_number_misscheduled{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeJobNotCompleted
      annotations:
        description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking
          more than {{ "43200" | humanizeDuration }} to complete.
        summary: Job did not complete in time
      expr: |
        time() - max by(namespace, job_name, cluster) (kube_job_status_start_time{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          and
        kube_job_status_active{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0) > 43200
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeJobFailed
      annotations:
        description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to
          complete. Removing failed job after investigation should clear this alert.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeJobFailed.md
        summary: Job failed to complete.
      expr: |
        kube_job_failed{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}  > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeHpaReplicasMismatch
      annotations:
        description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
          has not matched the desired number of replicas for longer than 15 minutes.
        summary: HPA has not matched desired number of replicas.
      expr: |
        (kube_horizontalpodautoscaler_status_desired_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          !=
        kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
          and
        (kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          >
        kube_horizontalpodautoscaler_spec_min_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
          and
        (kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          <
        kube_horizontalpodautoscaler_spec_max_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"})
          and
        changes(kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}[15m]) == 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeHpaMaxedOut
      annotations:
        description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
          has been running at max replicas for longer than 15 minutes.
        summary: HPA is running at max replicas
      expr: |
        kube_horizontalpodautoscaler_status_current_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
          ==
        kube_horizontalpodautoscaler_spec_max_replicas{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"}
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: kubernetes-resources
    rules:
    - alert: KubeCPUOvercommit
      annotations:
        description: Cluster {{ $labels.cluster }} has overcommitted CPU resource
          requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.
        summary: Cluster has overcommitted CPU resource requests.
      expr: |
        sum(namespace_cpu:kube_pod_container_resource_requests:sum{job="kube-state-metrics",}) by (cluster) - (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
        and
        (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
      for: 10m
      labels:
        #namespace: kube-system
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeMemoryOvercommit
      annotations:
        description: Cluster {{ $labels.cluster }} has overcommitted memory resource
          requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node
          failure.
        summary: Cluster has overcommitted memory resource requests.
      expr: |
        sum(namespace_memory:kube_pod_container_resource_requests:sum{}) by (cluster) - (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
        and
        (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
      for: 10m
      labels:
        #namespace: kube-system
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeQuotaAlmostFull
      annotations:
        description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
          }} of its {{ $labels.resource }} quota.
        summary: Namespace quota is going to be full.
      expr: |
        kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
          > 0.9 < 1
      for: 15m
      labels:
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeQuotaFullyUsed
      annotations:
        description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
          }} of its {{ $labels.resource }} quota.
        summary: Namespace quota is fully used.
      expr: |
        kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
          == 1
      for: 15m
      labels:
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeQuotaExceeded
      annotations:
        description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage
          }} of its {{ $labels.resource }} quota.
        summary: Namespace quota has exceeded the limits.
      expr: |
        kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="used"}
          / ignoring(instance, job, type)
        (kube_resourcequota{namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics", type="hard"} > 0)
          > 1
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: kubernetes-system
    rules:
    - alert: KubeClientErrors
      annotations:
        description: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance
          }}' is experiencing {{ $value | humanizePercentage }} errors.'
        summary: Kubernetes API server client is experiencing errors.
      expr: |
        (sum(rate(rest_client_requests_total{job="apiserver",code=~"5.."}[5m])) by (cluster, instance, job, namespace)
          /
        sum(rate(rest_client_requests_total{job="apiserver"}[5m])) by (cluster, instance, job, namespace))
        > 0.01
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: kubernetes-system-kubelet
    rules:
    - alert: KubeNodeNotReady
      annotations:
        description: '{{ $labels.node }} has been unready for more than 15 minutes.'
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeNodeNotReady.md
        summary: Node is not ready.
      expr: |
        kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeNodeUnreachable
      annotations:
        description: '{{ $labels.node }} is unreachable and some workloads may be
          rescheduled.'
        summary: Node is unreachable.
      expr: |
        (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"} unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"}) == 1
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeletTooManyPods
      annotations:
        description: Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage
          }} of its Pod capacity.
        summary: Kubelet is running at capacity.
      expr: |
        count by(cluster, node) (
          (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
        )
        /
        max by(cluster, node) (
          kube_node_status_capacity{job="kube-state-metrics",resource="pods"} != 1
        ) > 0.95
      for: 15m
      labels:
        #namespace: kube-system
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeNodeReadinessFlapping
      annotations:
        description: The readiness status of node {{ $labels.node }} has changed {{
          $value }} times in the last 15 minutes.
        summary: Node readiness status is flapping.
      expr: |
        sum(changes(kube_node_status_condition{job="kube-state-metrics",status="true",condition="Ready"}[15m])) by (cluster, node) > 2
      for: 15m
      labels:
        #namespace: kube-system
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeletPlegDurationHigh
      annotations:
        description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile
          duration of {{ $value }} seconds on node {{ $labels.node }}.
        summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
      expr: |
        node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
      for: 5m
      labels:
        #namespace: kube-system
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeletPodStartUpLatencyHigh
      annotations:
        description: Kubelet Pod startup 99th percentile latency is {{ $value }} seconds
          on node {{ $labels.node }}.
        summary: Kubelet Pod startup latency is too high.
      expr: |
        histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le)) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"} > 60
      for: 15m
      labels:
        #namespace: kube-system
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeletClientCertificateRenewalErrors
      annotations:
        description: Kubelet on node {{ $labels.node }} has failed to renew its client
          certificate ({{ $value | humanize }} errors in the last 5 minutes).
        summary: Kubelet has failed to renew its client certificate.
      expr: |
        increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeletServerCertificateRenewalErrors
      annotations:
        description: Kubelet on node {{ $labels.node }} has failed to renew its server
          certificate ({{ $value | humanize }} errors in the last 5 minutes).
        summary: Kubelet has failed to renew its server certificate.
      expr: |
        increase(kubelet_server_expiration_renew_errors[5m]) > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubeletDown
      annotations:
        description: Kubelet has disappeared from Prometheus target discovery.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeletDown.md
        summary: Target disappeared from Prometheus target discovery.
      expr: |
        absent(up{job="kubelet", metrics_path="/metrics"} == 1)
      for: 15m
      labels:
        #namespace: kube-system
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
  - name: k8s.rules.container_cpu_usage_seconds_total
    rules:
    - expr: |
        sum by (cluster, namespace, pod, container) (
          irate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
        ) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
          1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
  - name: k8s.rules.container_memory_working_set_bytes
    rules:
    - expr: |
        container_memory_working_set_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
          max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_working_set_bytes
  - name: k8s.rules.container_memory_rss
    rules:
    - expr: |
        container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
          max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_rss
  - name: k8s.rules.container_memory_cache
    rules:
    - expr: |
        container_memory_cache{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
          max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_cache
  - name: k8s.rules.container_memory_swap
    rules:
    - expr: |
        container_memory_swap{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
        * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
          max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
        )
      record: node_namespace_pod_container:container_memory_swap
  - name: k8s.rules.container_resource
    rules:
    - expr: |
        kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
        group_left() max by (namespace, pod, cluster) (
          (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
        )
      record: cluster:namespace:pod_memory:active:kube_pod_container_resource_requests
    - expr: |
        sum by (namespace, cluster) (
            sum by (namespace, pod, cluster) (
                max by (namespace, pod, container, cluster) (
                  kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}
                ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace_memory:kube_pod_container_resource_requests:sum
    - expr: |
        kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
        group_left() max by (namespace, pod, cluster) (
          (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
        )
      record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests
    - expr: |
        sum by (namespace, cluster) (
            sum by (namespace, pod, cluster) (
                max by (namespace, pod, container, cluster) (
                  kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}
                ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace_cpu:kube_pod_container_resource_requests:sum
    - expr: |
        kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
        group_left() max by (namespace, pod, cluster) (
          (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
        )
      record: cluster:namespace:pod_memory:active:kube_pod_container_resource_limits
    - expr: |
        sum by (namespace, cluster) (
            sum by (namespace, pod, cluster) (
                max by (namespace, pod, container, cluster) (
                  kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}
                ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace_memory:kube_pod_container_resource_limits:sum
    - expr: |
        kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
        group_left() max by (namespace, pod, cluster) (
         (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
         )
      record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits
    - expr: |
        sum by (namespace, cluster) (
            sum by (namespace, pod, cluster) (
                max by (namespace, pod, container, cluster) (
                  kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}
                ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                  kube_pod_status_phase{phase=~"Pending|Running"} == 1
                )
            )
        )
      record: namespace_cpu:kube_pod_container_resource_limits:sum
  - name: k8s.rules.pod_owner
    rules:
    - expr: |
        max by (cluster, namespace, workload, pod) (
          label_replace(
            label_replace(
              kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
              "replicaset", "$1", "owner_name", "(.*)"
            ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (
              1, max by (replicaset, namespace, owner_name) (
                kube_replicaset_owner{job="kube-state-metrics"}
              )
            ),
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: deployment
      record: namespace_workload_pod:kube_pod_owner:relabel
    - expr: |
        max by (cluster, namespace, workload, pod) (
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: daemonset
      record: namespace_workload_pod:kube_pod_owner:relabel
    - expr: |
        max by (cluster, namespace, workload, pod) (
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: statefulset
      record: namespace_workload_pod:kube_pod_owner:relabel
    - expr: |
        max by (cluster, namespace, workload, pod) (
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="Job"},
            "workload", "$1", "owner_name", "(.*)"
          )
        )
      labels:
        workload_type: job
      record: namespace_workload_pod:kube_pod_owner:relabel
  - name: kube-scheduler.rules
    rules:
    - expr: |
        histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.99"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.99"
      record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.99"
      record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.9"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.9"
      record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.9"
      record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.5"
      record: cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.5"
      record: cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job="scheduler"}[5m])) without(instance, pod))
      labels:
        quantile: "0.5"
      record: cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile
  - name: node.rules
    rules:
    - expr: |
        topk by(cluster, namespace, pod) (1,
          max by (cluster, node, namespace, pod) (
            label_replace(kube_pod_info{job="kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
        ))
      record: 'node_namespace_pod:kube_pod_info:'
    - expr: |
        sum(
          node_memory_MemAvailable_bytes{job="node-exporter"} or
          (
            node_memory_Buffers_bytes{job="node-exporter"} +
            node_memory_Cached_bytes{job="node-exporter"} +
            node_memory_MemFree_bytes{job="node-exporter"} +
            node_memory_Slab_bytes{job="node-exporter"}
          )
        ) by (cluster)
      record: :node_memory_MemAvailable_bytes:sum
    - expr: |
        avg by (cluster, node) (
          sum without (mode) (
            rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal",job="node-exporter"}[5m])
          )
        )
      record: node:node_cpu_utilization:ratio_rate5m
    - expr: |
        avg by (cluster) (
          node:node_cpu_utilization:ratio_rate5m
        )
      record: cluster:node_cpu:ratio_rate5m
  - name: kubelet.rules
    rules:
    - expr: |
        histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      labels:
        quantile: "0.99"
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      labels:
        quantile: "0.9"
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
    - expr: |
        histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job="kubelet", metrics_path="/metrics"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"})
      labels:
        quantile: "0.5"
      record: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile
---
# Source: cluster-monitoring/templates/prometheusrules/machine-config-controller.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: machine-config-controller
spec:
  groups:
  - name: os-image-override.rules
    rules:
    - expr: sum(os_image_url_override)
      record: os_image_url_override:sum
  - name: mcc-drain-error
    rules:
    - alert: MCCDrainError
      annotations:
        description: 'Drain failed on {{ $labels.exported_node }} , updates may be
          blocked. For more details check MachineConfigController pod logs: oc logs
          -f -n {{ $labels.namespace }} machine-config-controller-xxxxx -c machine-config-controller'
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/machine-config-operator/MachineConfigControllerDrainError.md
        summary: Alerts the user to a failed node drain. Always triggers when the
          failure happens one or more times.
      expr: |
        mcc_drain_err > 0
      labels:
        #namespace: openshift-machine-config-operator
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: mcc-pool-alert
    rules:
    - alert: MCCPoolAlert
      annotations:
        description: 'Node {{ $labels.exported_node }} has triggered a pool alert
          due to a label change. For more details check MachineConfigController pod
          logs: oc logs -f -n {{ $labels.namespace }} machine-config-controller-xxxxx
          -c machine-config-controller'
        summary: Triggers when nodes in a pool have overlapping labels such as master,
          worker, and a custom label therefore a choice must be made as to which is
          honored.
      expr: |
        mcc_pool_alert > 0
      labels:
        #namespace: openshift-machine-config-operator
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/machine-config-daemon.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: machine-config-daemon
spec:
  groups:
  - name: mcd-reboot-error
    rules:
    - alert: MCDRebootError
      annotations:
        description: 'Reboot failed on {{ $labels.node }} , update may be blocked.
          For more details:  oc logs -f -n {{ $labels.namespace }} {{ $labels.pod
          }} -c machine-config-daemon '
        summary: Alerts the user that a node failed to reboot one or more times over
          a span of 5 minutes.
      expr: |
        mcd_reboots_failed_total > 0
      for: 5m
      labels:
        #namespace: openshift-machine-config-operator
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
  - name: mcd-pivot-error
    rules:
    - alert: MCDPivotError
      annotations:
        description: 'Error detected in pivot logs on {{ $labels.node }} , upgrade
          may be blocked. For more details:  oc logs -f -n {{ $labels.namespace }}
          {{ $labels.pod }} -c machine-config-daemon '
        summary: Alerts the user when an error is detected upon pivot. This triggers
          if the pivot errors are above zero for 2 minutes.
      expr: |
        mcd_pivot_errors_total > 0
      for: 2m
      labels:
        #namespace: openshift-machine-config-operator
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: mcd-kubelet-health-state-error
    rules:
    - alert: KubeletHealthState
      annotations:
        description: Kubelet health failure threshold reached
        summary: This keeps track of Kubelet health failures, and tallys them. The
          warning is triggered if 2 or more failures occur.
      expr: |
        mcd_kubelet_state > 2
      labels:
        #namespace: openshift-machine-config-operator
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: system-memory-exceeds-reservation
    rules:
    - alert: SystemMemoryExceedsReservation
      annotations:
        description: System memory usage of {{ $value | humanize }} on {{ $labels.node
          }} exceeds 95% of the reservation. Reserved memory ensures system processes
          can function even when the node is fully allocated and protects against
          workload out of memory events impacting the proper functioning of the node.
          The default reservation is expected to be sufficient for most configurations
          and should be increased (https://docs.openshift.com/container-platform/latest/nodes/nodes/nodes-nodes-managing.html)
          when running nodes with high numbers of pods (either due to rate of change
          or at steady state).
        summary: Alerts the user when, for 15 miutes, a specific node is using more
          memory than is reserved
      expr: |
        sum by (node) (container_memory_rss{id="/system.slice"}) > ((sum by (node) (kube_node_status_capacity{resource="memory"} - kube_node_status_allocatable{resource="memory"})) * 0.95)
      for: 15m
      labels:
        #namespace: openshift-machine-config-operator
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: high-overall-control-plane-memory
    rules:
    - alert: HighOverallControlPlaneMemory
      annotations:
        description: Given three control plane nodes, the overall memory utilization
          may only be about 2/3 of all available capacity. This is because if a single
          control plane node fails, the kube-apiserver and etcd may be slow to respond.
          To fix this, increase memory of the control plane nodes.
        summary: Memory utilization across all control plane nodes is high, and could
          impact responsiveness and stability.
      expr: |
        (
          1
          -
          sum (
            node_memory_MemFree_bytes
            + node_memory_Buffers_bytes
            + node_memory_Cached_bytes
            AND on (instance)
            label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
          ) / sum (
            node_memory_MemTotal_bytes
            AND on (instance)
            label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
          )
        ) * 100 > 60
      for: 1h
      labels:
        #namespace: openshift-machine-config-operator
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: extremely-high-individual-control-plane-memory
    rules:
    - alert: ExtremelyHighIndividualControlPlaneMemory
      annotations:
        description: The memory utilization per instance within control plane nodes
          influence the stability, and responsiveness of the cluster. This can lead
          to cluster instability and slow responses from kube-apiserver or failing
          requests specially on etcd. Moreover, OOM kill is expected which negatively
          influences the pod scheduling. If this happens on container level, the descheduler
          will not be able to detect it, as it works on the pod level. To fix this,
          increase memory of the affected node of control plane nodes.
        summary: Extreme memory utilization per node within control plane nodes is
          extremely high, and could impact responsiveness and stability.
      expr: |
        (
          1
          -
          sum by (instance) (
            node_memory_MemFree_bytes
            + node_memory_Buffers_bytes
            + node_memory_Cached_bytes
            AND on (instance)
            label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
          ) / sum by (instance) (
            node_memory_MemTotal_bytes
            AND on (instance)
            label_replace( kube_node_role{role="master"}, "instance", "$1", "node", "(.+)" )
          )
        ) * 100 > 90
      for: 45m
      labels:
        #namespace: openshift-machine-config-operator
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
  - name: mcd-missing-mc
    rules:
    - alert: MissingMachineConfig
      annotations:
        description: Could not find config {{ $labels.mc }} in-cluster, this likely
          indicates the MachineConfigs in-cluster has changed during the install process.  If
          you are seeing this when installing the cluster, please compare the in-cluster
          rendered machineconfigs to /etc/mcs-machine-config-content.json
        summary: This keeps track of Machine Config failures. Specifically a common
          failure on install when a rendered Machine Config is missing. Triggered
          when this error happens once.
      expr: |
        mcd_missing_mc > 0
      labels:
        #namespace: openshift-machine-config-operator
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: telemetry.rules
    rules:
    - expr: count(mcd_local_unsupported_packages > 0)
      record: cluster:mcd_nodes_with_unsupported_packages:count
    - expr: sum(mcd_local_unsupported_packages)
      record: cluster:mcd_total_unsupported_packages:sum
---
# Source: cluster-monitoring/templates/prometheusrules/machine-config-operator.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: machine-config-operator
spec:
  groups:
  - name: drain-override-configmap-present
    rules:
    - alert: MCODrainOverrideConfigMapAlert
      annotations:
        description: Image Registry Drain Override configmap has been detected. Please
          use the Node Disruption Policy feature to control the cluster's drain behavior
          as the configmap method is currently deprecated and will be removed in a
          future release.
        summary: Alerts the user to the presence of a drain override configmap that
          is being deprecated and removed in a future release.
      expr: |
        mco_image_registry_drain_override_exists > 0
      labels:
        #namespace: openshift-machine-config-operator
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/marketplace-alert-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: marketplace-alert-rules
spec:
  groups:
  - name: operator.marketplace.rules
    rules:
    - alert: OperatorHubSourceError
      annotations:
        description: Operators shipped via the {{ $labels.name }} source are not available
          for installation until the issue is fixed. Operators already installed from
          this source will not receive updates until issue is fixed. Inspect the status
          of the pod owned by {{ $labels.name }} source in the openshift-marketplace
          namespace (oc -n openshift-marketplace get pods -l olm.catalogSource={{
          $labels.name }}) to diagnose and repair.
        summary: The {{ $labels.name }} source is in non-ready state for more than
          10 minutes.
      expr: catalogsource_ready{exported_namespace="openshift-marketplace"} == 0
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/networking-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: networking-rules
spec:
  groups:
  - name: cluster-network-operator-ovn.rules
    rules:
    - alert: NodeWithoutOVNKubeNodePodRunning
      annotations:
        description: |
          Networking is degraded on nodes that do not have a functioning ovnkube-node pod. Existing workloads on the
          node may continue to have connectivity but any changes to the networking control plane will not be implemented.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NodeWithoutOVNKubeNodePodRunning.md
        summary: All Linux nodes should be running an ovnkube-node pod, {{ $labels.node
          }} is not.
      expr: |
        (kube_node_info unless on(node) (kube_pod_info{namespace="openshift-ovn-kubernetes",pod=~"ovnkube-node.*"}
        or kube_node_labels{label_kubernetes_io_os="windows"})) > 0
      for: 20m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: OVNKubernetesControllerDisconnectedSouthboundDatabase
      annotations:
        description: |
          Networking is degraded on nodes when OVN controller is not connected to OVN southbound database connection. No networking control plane updates will be applied to the node.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/OVNKubernetesControllerDisconnectedSouthboundDatabase.md
        summary: Networking control plane is degraded on node {{ $labels.node }} because
          OVN controller is not connected to OVN southbound database.
      expr: |
        max_over_time(ovn_controller_southbound_database_connected[5m]) == 0
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: OVNKubernetesNodePodAddError
      annotations:
        description: OVN Kubernetes experiences pod creation errors at an elevated
          rate. The pods will be retried.
        summary: OVN Kubernetes is experiencing pod creation errors at an elevated
          rate.
      expr: |
        (sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command="ADD",err="true"}[5m]))
          /
        sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command="ADD"}[5m])))
        > 0.1
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: OVNKubernetesNodePodDeleteError
      annotations:
        description: OVN Kubernetes experiences pod deletion errors at an elevated
          rate. The pods will be retried.
        summary: OVN Kubernetes experiencing pod deletion errors at an elevated rate.
      expr: |
        (sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command="DEL",err="true"}[5m]))
          /
        sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command="DEL"}[5m])))
        > 0.1
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: OVNKubernetesResourceRetryFailure
      annotations:
        description: |
          OVN Kubernetes failed to apply networking control plane configuration after several attempts. This might be because the configuration
          provided by the user is invalid or because of an internal error. As a consequence, the cluster might have a degraded status.
        summary: OVN Kubernetes failed to apply networking control plane configuration.
      expr: increase(ovnkube_resource_retry_failures_total[10m]) > 0
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: OVNKubernetesNodeOVSOverflowUserspace
      annotations:
        description: Netlink messages dropped by OVS vSwitch daemon due to netlink
          socket buffer overflow. This will result in packet loss.
        summary: OVS vSwitch daemon drops packets due to buffer overflow.
      expr: increase(ovs_vswitchd_netlink_overflow[5m]) > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: OVNKubernetesNodeOVSOverflowKernel
      annotations:
        description: Netlink messages dropped by OVS kernel module due to netlink
          socket buffer overflow. This will result in packet loss.
        summary: OVS kernel module drops packets due to buffer overflow.
      expr: increase(ovs_vswitchd_dp_flows_lookup_lost[5m]) > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NorthboundStale
      annotations:
        description: |
          OVN-Kubernetes controller and/or OVN northbound database may cause a
          degraded networking control plane for the affected node. Existing
          workloads should continue to have connectivity but new workloads may
          be impacted.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NorthboundStaleAlert.md
        summary: OVN-Kubernetes controller {{ $labels.instance }} has not successfully
          synced any changes to the northbound database for too long.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        time() - max_over_time(ovnkube_controller_nb_e2e_timestamp[5m]) > 120
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: SouthboundStale
      annotations:
        description: |
          OVN-Kubernetes controller and/or OVN northbound database may cause a
          degraded networking control plane for the affected node. Existing
          workloads should continue to have connectivity but new workloads may
          be impacted.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/SouthboundStaleAlert.md
        summary: OVN northd {{ $labels.instance }} has not successfully synced any
          changes to the southbound database for too long.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(ovnkube_controller_nb_e2e_timestamp[5m]) - max_over_time(ovnkube_controller_sb_e2e_timestamp[5m]) > 120
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: OVNKubernetesNorthboundDatabaseCPUUsageHigh
      annotations:
        description: |
          High OVN northbound CPU usage indicates high load on the networking
          control plane for the affected node.
        summary: OVN northbound database {{ $labels.instance }} is greater than {{
          $value | humanizePercentage }} percent CPU usage for a period of time.
      expr: (sum(rate(container_cpu_usage_seconds_total{container="nbdb"}[5m])) BY
        (instance, name, namespace)) > 0.8
      for: 15m
      labels:
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: OVNKubernetesSouthboundDatabaseCPUUsageHigh
      annotations:
        description: |
          High OVN southbound CPU usage indicates high load on the networking
          control plane for the affected node.
        summary: OVN southbound database {{ $labels.instance }} is greater than {{
          $value | humanizePercentage }} percent CPU usage for a period of time.
      expr: (sum(rate(container_cpu_usage_seconds_total{container="sbdb"}[5m])) BY
        (instance, name, namespace)) > 0.8
      for: 15m
      labels:
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: OVNKubernetesNorthdInactive
      annotations:
        description: |
          An inactive OVN northd instance may cause a degraded networking
          control plane for the affected node. Existing workloads should
          continue to have connectivity but new workloads may be impacted.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/OVNKubernetesNorthdInactive.md
        summary: OVN northd {{ $labels.instance }} is not active.
      expr: count(ovn_northd_status != 1) BY (instance, name, namespace) > 0
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/node-exporter-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: node-exporter-rules
spec:
  groups:
  - name: node-exporter
    rules:
    - alert: NodeFilesystemSpaceFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          space left and is filling up.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md
        summary: Filesystem is predicted to run out of space within the next 24 hours.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 15
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeFilesystemSpaceFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          space left and is filling up fast.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md
        summary: Filesystem is predicted to run out of space within the next 4 hours.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 10
        and
          predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 1h
      labels:
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          space left.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md
        summary: Filesystem has less than 5% space left.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 30m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeFilesystemAlmostOutOfSpace
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          space left.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md
        summary: Filesystem has less than 3% space left.
      expr: |
        (
          node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 30m
      labels:
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeFilesystemFilesFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          inodes left and is filling up.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md
        summary: Filesystem is predicted to run out of inodes within the next 24 hours.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 40
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 24*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeFilesystemFilesFillingUp
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          inodes left and is filling up fast.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md
        summary: Filesystem is predicted to run out of inodes within the next 4 hours.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 20
        and
          predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"}[6h], 4*60*60) < 0
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 1h
      labels:
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeFilesystemAlmostOutOfFiles
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          inodes left.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md
        summary: Filesystem has less than 5% inodes left.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 5
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeFilesystemAlmostOutOfFiles
      annotations:
        description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint
          }}, at {{ $labels.instance }} has only {{ printf "%.2f" $value }}% available
          inodes left.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md
        summary: Filesystem has less than 3% inodes left.
      expr: |
        (
          node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} * 100 < 3
        and
          node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!~"/var/lib/ibmc-s3fs.*"} == 0
        )
      for: 1h
      labels:
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeNetworkReceiveErrs
      annotations:
        description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
          {{ printf "%.0f" $value }} receive errors in the last two minutes.'
        summary: Network interface is reporting many receive errors.
      expr: |
        rate(node_network_receive_errs_total{job="node-exporter"}[2m]) / rate(node_network_receive_packets_total{job="node-exporter"}[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeNetworkTransmitErrs
      annotations:
        description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered
          {{ printf "%.0f" $value }} transmit errors in the last two minutes.'
        summary: Network interface is reporting many transmit errors.
      expr: |
        rate(node_network_transmit_errs_total{job="node-exporter"}[2m]) / rate(node_network_transmit_packets_total{job="node-exporter"}[2m]) > 0.01
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeHighNumberConntrackEntriesUsed
      annotations:
        description: '{{ $value | humanizePercentage }} of conntrack entries are used.'
        summary: Number of conntrack are getting close to the limit.
      expr: |
        (node_nf_conntrack_entries{job="node-exporter"} / node_nf_conntrack_entries_limit) > 0.75
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeTextFileCollectorScrapeError
      annotations:
        description: Node Exporter text file collector on {{ $labels.instance }} failed
          to scrape.
        summary: Node Exporter text file collector failed to scrape.
      expr: |
        node_textfile_scrape_error{job="node-exporter"} == 1
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeClockSkewDetected
      annotations:
        description: Clock at {{ $labels.instance }} is out of sync by more than 0.05s.
          Ensure NTP is configured correctly on this host.
        summary: Clock skew detected.
      expr: |-
        (
        (
          node_timex_offset_seconds{job="node-exporter"} > 0.05
        and
          deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) >= 0
        )
        or
        (
          node_timex_offset_seconds{job="node-exporter"} < -0.05
        and
          deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) <= 0
        )
        ) and on() absent(up{job="ptp-monitor-service"})
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeClockNotSynchronising
      annotations:
        description: Clock at {{ $labels.instance }} is not synchronising. Ensure
          NTP is configured on this host.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeClockNotSynchronising.md
        summary: Clock not synchronising.
      expr: |-
        (
        min_over_time(node_timex_sync_status{job="node-exporter"}[5m]) == 0
        and
        node_timex_maxerror_seconds{job="node-exporter"} >= 16
        ) and on() absent(up{job="ptp-monitor-service"})
      for: 10m
      labels:
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeRAIDDegraded
      annotations:
        description: RAID array '{{ $labels.device }}' at {{ $labels.instance }} is
          in degraded state due to one or more disks failures. Number of spare drives
          is insufficient to fix issue automatically.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeRAIDDegraded.md
        summary: RAID Array is degraded.
      expr: |
        node_md_disks_required{job="node-exporter",device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"} - ignoring (state) (node_md_disks{state="active",job="node-exporter",device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}) > 0
      for: 15m
      labels:
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeRAIDDiskFailure
      annotations:
        description: At least one device in RAID array at {{ $labels.instance }} failed.
          Array '{{ $labels.device }}' needs attention and possibly a disk swap.
        summary: Failed device in RAID array.
      expr: |
        node_md_disks{state="failed",job="node-exporter",device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"} > 0
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeFileDescriptorLimit
      annotations:
        description: File descriptors limit at {{ $labels.instance }} is currently
          at {{ printf "%.2f" $value }}%.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md
        summary: Kernel is predicted to exhaust file descriptors limit soon.
      expr: |
        (
          node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 70
        )
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeFileDescriptorLimit
      annotations:
        description: File descriptors limit at {{ $labels.instance }} is currently
          at {{ printf "%.2f" $value }}%.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md
        summary: Kernel is predicted to exhaust file descriptors limit soon.
      expr: |
        (
          node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 90
        )
      for: 15m
      labels:
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeSystemSaturation
      annotations:
        description: |
          System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
          This might indicate this instance resources saturation and can cause it becoming unresponsive.
        summary: System saturated, load per core is very high.
      expr: |
        node_load1{job="node-exporter"}
        / count without (cpu, mode) (node_cpu_seconds_total{job="node-exporter", mode="idle"}) > 2
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeMemoryMajorPagesFaults
      annotations:
        description: |
          Memory major pages are occurring at very high rate at {{ $labels.instance }}, 500 major page faults per second for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
          Please check that there is enough memory available at this instance.
        summary: Memory major page faults are occurring at very high rate.
      expr: |
        rate(node_vmstat_pgmajfault{job="node-exporter"}[5m]) > 500
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeSystemdServiceFailed
      annotations:
        description: Systemd service {{ $labels.name }} has entered failed state at
          {{ $labels.instance }}
        summary: Systemd service has entered failed state.
      expr: |
        node_systemd_unit_state{job="node-exporter", state="failed"} == 1
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NodeBondingDegraded
      annotations:
        description: Bonding interface {{ $labels.master }} on {{ $labels.instance
          }} is in degraded state due to one or more slave failures.
        summary: Bonding interface is degraded
      expr: |
        (node_bonding_slaves - node_bonding_active) != 0
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: node-exporter.rules
    rules:
    - expr: |
        count without (cpu, mode) (
          node_cpu_seconds_total{job="node-exporter",mode="idle"}
        )
      record: instance:node_num_cpu:sum
    - expr: |
        1 - avg without (cpu) (
          sum without (mode) (rate(node_cpu_seconds_total{job="node-exporter", mode=~"idle|iowait|steal"}[1m]))
        )
      record: instance:node_cpu_utilisation:rate1m
    - expr: |
        (
          node_load1{job="node-exporter"}
        /
          instance:node_num_cpu:sum{job="node-exporter"}
        )
      record: instance:node_load1_per_cpu:ratio
    - expr: |
        1 - (
          (
            node_memory_MemAvailable_bytes{job="node-exporter"}
            or
            (
              node_memory_Buffers_bytes{job="node-exporter"}
              +
              node_memory_Cached_bytes{job="node-exporter"}
              +
              node_memory_MemFree_bytes{job="node-exporter"}
              +
              node_memory_Slab_bytes{job="node-exporter"}
            )
          )
        /
          node_memory_MemTotal_bytes{job="node-exporter"}
        )
      record: instance:node_memory_utilisation:ratio
    - expr: |
        rate(node_vmstat_pgmajfault{job="node-exporter"}[1m])
      record: instance:node_vmstat_pgmajfault:rate1m
    - expr: |
        rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
      record: instance_device:node_disk_io_time_seconds:rate1m
    - expr: |
        rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+"}[1m])
      record: instance_device:node_disk_io_time_weighted_seconds:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_receive_bytes_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_transmit_bytes_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_receive_drop_excluding_lo:rate1m
    - expr: |
        sum without (device) (
          rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[1m])
        )
      record: instance:node_network_transmit_drop_excluding_lo:rate1m
  - name: telemetry
    rules:
    - expr: sum by(vendor,model) (node_accelerator_card_info)
      record: vendor_model:node_accelerator_cards:sum
---
# Source: cluster-monitoring/templates/prometheusrules/node-tuning-operator.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: node-tuning-operator
spec:
  groups:
  - name: node-tuning-operator.rules
    rules:
    - alert: NTOPodsNotReady
      annotations:
        description: |
          Pod {{ $labels.pod }} is not ready.
          Review the "Event" objects in "openshift-cluster-node-tuning-operator" namespace for further details.
        summary: Pod {{ $labels.pod }} is not ready.
      expr: |
        kube_pod_status_ready{namespace='openshift-cluster-node-tuning-operator', condition='true'} == 0
      for: 30m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: NTODegraded
      annotations:
        description: The Node Tuning Operator is degraded. Review the "node-tuning"
          ClusterOperator object for further details.
        summary: The Node Tuning Operator is degraded.
      expr: nto_degraded_info == 1
      for: 2h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - expr: count by (_id) (nto_profile_calculated_total{profile!~"openshift-node",profile!~"openshift-control-plane",profile!~"openshift"})
      record: nto_custom_profiles:count
---
# Source: cluster-monitoring/templates/prometheusrules/olm-alert-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: olm-alert-rules
spec:
  groups:
  - name: olm.csv_abnormal.rules
    rules:
    - alert: CsvAbnormalFailedOver2Min
      annotations:
        description: Failed to install Operator {{ $labels.name }} version {{ $labels.version
          }}. Reason-{{ $labels.reason }}
        summary: CSV failed for over 2 minutes
      expr: last_over_time(csv_abnormal{phase="Failed"}[5m])
      for: 2m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: CsvAbnormalOver30Min
      annotations:
        description: Failed to install Operator {{ $labels.name }} version {{ $labels.version
          }}. Phase-{{ $labels.phase }} Reason-{{ $labels.reason }}
        summary: CSV abnormal for over 30 minutes
      expr: last_over_time(csv_abnormal{phase=~"(Replacing|Pending|Deleting|Unknown)"}[5m])
      for: 30m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: olm.installplan.rules
    rules:
    - alert: InstallPlanStepAppliedWithWarnings
      annotations:
        description: The API server returned a warning during installation or upgrade
          of an operator. An Event with reason "AppliedWithWarnings" has been created
          with complete details, including a reference to the InstallPlan step that
          generated the warning.
        summary: API returned a warning when modifying an operator
      expr: sum by(namespace) (increase(installplan_warnings_total[5m])) > 0
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/openshift-network-operator-ipsec-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: openshift-network-operator-ipsec-rules
spec:
  groups:
  - name: openshift-network.rules
    rules:
    - expr: |-
        group by (mode,is_legacy_api) (
          openshift_network_operator_ipsec_state{namespace=~"openshift-network-operator"}
        )
      record: openshift:openshift_network_operator_ipsec_state:info
---
# Source: cluster-monitoring/templates/prometheusrules/podsecurity.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: podsecurity
spec:
  groups:
  - name: pod-security-violation
    rules:
    - alert: PodSecurityViolation
      annotations:
        description: A workload (pod, deployment, daemonset, ...) was created somewhere
          in the cluster but it did not match the PodSecurity "{{ $labels.policy_level
          }}" profile defined by its namespace either via the cluster-wide configuration
          (which triggers on a "restricted" profile violations) or by the namespace
          local Pod Security labels. Refer to Kubernetes documentation on Pod Security
          Admission to learn more about these violations.
        summary: One or more workloads users created in the cluster don't match their
          Pod Security profile
      expr: |
        sum(increase(pod_security_evaluations_total{decision="deny",mode="audit",resource="pod",ocp_namespace=""}[1d])) by (policy_level, ocp_namespace) > 0
      labels:
        #namespace: openshift-kube-apiserver
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: PodSecurityViolation
      annotations:
        description: A workload (pod, deployment, daemonset, ...) was created in namespace
          "{{ $labels.ocp_namespace }}" but it did not match the PodSecurity "{{ $labels.policy_level
          }}" profile defined by its namespace either via the cluster-wide configuration
          (which triggers on a "restricted" profile violations) or by the namespace
          local Pod Security labels. Refer to Kubernetes documentation on Pod Security
          Admission to learn more about these violations.
        summary: One or more workloads in platform namespaces of the cluster don't
          match their Pod Security profile
      expr: |
        sum(increase(pod_security_evaluations_total{decision="deny",mode="audit",resource="pod",ocp_namespace!=""}[1d])) by (policy_level, ocp_namespace) > 0
      labels:
        #namespace: openshift-kube-apiserver
        severity: info
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/prometheus-k8s-prometheus-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
    namespace: cluster-monitoring
  namespace: cluster-monitoring
  name: prometheus-k8s-prometheus-rules
spec:
  groups:
  - name: prometheus
    rules:
    - alert: PrometheusBadConfig
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
          reload its configuration.
        summary: Failed Prometheus configuration reload.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_config_last_reload_successful{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) == 0
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusSDRefreshFailure
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
          refresh SD with mechanism {{$labels.mechanism}}.
        summary: Failed Prometheus SD refresh.
      expr: |
        increase(prometheus_sd_refresh_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[10m]) > 0
      for: 20m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusKubernetesListWatchFailures
      annotations:
        description: Kubernetes service discovery of Prometheus {{$labels.namespace}}/{{$labels.pod}}
          is experiencing {{ printf "%.0f" $value }} failures with LIST/WATCH requests
          to the Kubernetes API in the last 5 minutes.
        summary: Requests in Kubernetes SD are failing.
      expr: |
        increase(prometheus_sd_kubernetes_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusNotificationQueueRunningFull
      annotations:
        description: Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}}
          is running full.
        summary: Prometheus alert notification queue predicted to run full in less
          than 30m.
      expr: |
        # Without min_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          predict_linear(prometheus_notifications_queue_length{job=~"prometheus-k8s|prometheus-user-workload"}[5m], 60 * 30)
        >
          min_over_time(prometheus_notifications_queue_capacity{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        )
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
      annotations:
        description: '{{ printf "%.1f" $value }}% errors while sending alerts from
          Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.'
        summary: Prometheus has encountered more than 1% errors sending alerts to
          a specific Alertmanager.
      expr: |
        (
          rate(prometheus_notifications_errors_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        /
          rate(prometheus_notifications_sent_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        )
        * 100
        > 1
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusNotConnectedToAlertmanagers
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected
          to any Alertmanagers.
        summary: Prometheus is not connected to any Alertmanagers.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        max_over_time(prometheus_notifications_alertmanagers_discovered{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) < 1
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusTSDBReloadsFailing
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected
          {{$value | humanize}} reload failures over the last 3h.
        summary: Prometheus has issues reloading blocks from disk.
      expr: |
        increase(prometheus_tsdb_reloads_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[3h]) > 0
      for: 4h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusTSDBCompactionsFailing
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected
          {{$value | humanize}} compaction failures over the last 3h.
        summary: Prometheus has issues compacting blocks.
      expr: |
        increase(prometheus_tsdb_compactions_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[3h]) > 0
      for: 4h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusNotIngestingSamples
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting
          samples.
        summary: Prometheus is not ingesting samples.
      expr: |
        (
          sum without(type) (rate(prometheus_tsdb_head_samples_appended_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m])) <= 0
        and
          (
            sum without(scrape_job) (prometheus_target_metadata_cache_entries{job=~"prometheus-k8s|prometheus-user-workload"}) > 0
          or
            sum without(rule_group) (prometheus_rule_group_rules{job=~"prometheus-k8s|prometheus-user-workload"}) > 0
          )
        )
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusDuplicateTimestamps
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping
          {{ printf "%.4g" $value  }} samples/s with different values but duplicated
          timestamp.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusDuplicateTimestamps.md
        summary: Prometheus is dropping samples with duplicate timestamps.
      expr: |
        rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusOutOfOrderTimestamps
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping
          {{ printf "%.4g" $value  }} samples/s with timestamps arriving out of order.
        summary: Prometheus drops samples with out-of-order timestamps.
      expr: |
        rate(prometheus_target_scrapes_sample_out_of_order_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusRemoteStorageFailures
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send
          {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{
          $labels.url }}
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusRemoteStorageFailures.md
        summary: Prometheus fails to send samples to remote storage.
      expr: |
        (
          (rate(prometheus_remote_storage_failed_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
        /
          (
            (rate(prometheus_remote_storage_failed_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
          +
            (rate(prometheus_remote_storage_succeeded_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) or rate(prometheus_remote_storage_samples_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]))
          )
        )
        * 100
        > 1
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusRemoteWriteBehind
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write
          is {{ printf "%.1f" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url
          }}.
        summary: Prometheus remote write is behind.
      expr: |
        # Use the metric added in https://github.com/openshift/prometheus/pull/262 and related PRs.
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          max_over_time(prometheus_remote_storage_queue_highest_timestamp_seconds{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        -
          max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        )
        > 120
      for: 15m
      labels:
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusRemoteWriteDesiredShards
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write
          desired shards calculation wants to run {{ $value }} shards for queue {{
          $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{
          printf `prometheus_remote_storage_shards_max{instance="%s",job=~"prometheus-k8s|prometheus-user-workload"}`
          $labels.instance | query | first | value }}.
        summary: Prometheus remote write desired shards calculation wants to run more
          than configured max shards.
      expr: |
        # Without max_over_time, failed scrapes could create false negatives, see
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
        (
          max_over_time(prometheus_remote_storage_shards_desired{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        >
          max_over_time(prometheus_remote_storage_shards_max{job=~"prometheus-k8s|prometheus-user-workload"}[5m])
        )
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusRuleFailures
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to
          evaluate {{ printf "%.0f" $value }} rules in the last 5m.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusRuleFailures.md
        summary: Prometheus is failing rule evaluations.
      expr: |
        increase(prometheus_rule_evaluation_failures_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusMissingRuleEvaluations
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{
          printf "%.0f" $value }} rule group evaluations in the last 5m.
        summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
      expr: |
        increase(prometheus_rule_group_iterations_missed_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusTargetLimitHit
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped
          {{ printf "%.0f" $value }} targets because the number of targets exceeded
          the configured target_limit.
        summary: Prometheus has dropped targets because some scrape configs have exceeded
          the targets limit.
      expr: |
        increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusLabelLimitHit
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped
          {{ printf "%.0f" $value }} targets because some samples exceeded the configured
          label_limit, label_name_length_limit or label_value_length_limit.
        summary: Prometheus has dropped targets because some scrape configs have exceeded
          the labels limit.
      expr: |
        increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusScrapeBodySizeLimitHit
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{
          printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded
          the configured body_size_limit.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusScrapeBodySizeLimitHit.md
        summary: Prometheus has dropped some targets that exceeded body size limit.
      expr: |
        increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusScrapeSampleLimitHit
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{
          printf "%.0f" $value }} scrapes in the last 5m because some targets exceeded
          the configured sample_limit.
        summary: Prometheus has failed scrapes that have exceeded the configured sample
          limit.
      expr: |
        increase(prometheus_target_scrapes_exceeded_sample_limit_total{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusTargetSyncFailure
      annotations:
        description: '{{ printf "%.0f" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}}
          have failed to sync because invalid configuration was supplied.'
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusTargetSyncFailure.md
        summary: Prometheus has failed to sync targets.
      expr: |
        increase(prometheus_target_sync_failed_total{job=~"prometheus-k8s|prometheus-user-workload"}[30m]) > 0
      for: 5m
      labels:
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusHighQueryLoad
      annotations:
        description: Prometheus {{$labels.namespace}}/{{$labels.pod}} query API has
          less than 20% available capacity in its query engine for the last 15 minutes.
        summary: Prometheus is reaching its maximum capacity serving concurrent requests.
      expr: |
        avg_over_time(prometheus_engine_queries{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) / max_over_time(prometheus_engine_queries_concurrent_max{job=~"prometheus-k8s|prometheus-user-workload"}[5m]) > 0.8
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/prometheus-k8s-thanos-sidecar-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  name: prometheus-k8s-thanos-sidecar-rules
spec:
  groups:
  - name: thanos-sidecar
    rules:
    - alert: ThanosSidecarBucketOperationsFailed
      annotations:
        description: Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}}
          bucket operations are failing
        summary: Thanos Sidecar bucket operations are failing
      expr: |
        sum by (namespace, job, instance) (rate(thanos_objstore_bucket_operation_failures_total{job=~"prometheus-(k8s|user-workload)-thanos-sidecar"}[5m])) > 0
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosSidecarNoConnectionToStartedPrometheus
      annotations:
        description: Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}}
          is unhealthy.
        summary: Thanos Sidecar cannot access Prometheus, even though Prometheus seems
          healthy and has reloaded WAL.
      expr: |
        thanos_sidecar_prometheus_up{job=~"prometheus-(k8s|user-workload)-thanos-sidecar"} == 0
        AND on (namespace, pod)
        prometheus_tsdb_data_replay_duration_seconds != 0
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/prometheus-operator-rules.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: prometheus-operator-rules
spec:
  groups:
  - name: prometheus-operator
    rules:
    - alert: PrometheusOperatorListErrors
      annotations:
        description: Errors while performing List operations in controller {{$labels.controller}}
          in {{$labels.namespace}} namespace.
        summary: Errors while performing list operations in controller.
      expr: |
        (sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_failed_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[10m])) / sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[10m]))) > 0.4
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusOperatorWatchErrors
      annotations:
        description: Errors while performing watch operations in controller {{$labels.controller}}
          in {{$labels.namespace}} namespace.
        summary: Errors while performing watch operations in controller.
      expr: |
        (sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_failed_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m])) / sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) > 0.4
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusOperatorSyncFailed
      annotations:
        description: Controller {{ $labels.controller }} in {{ $labels.namespace }}
          namespace fails to reconcile {{ $value }} objects.
        summary: Last controller reconciliation failed
      expr: |
        min_over_time(prometheus_operator_syncs{status="failed",job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusOperatorReconcileErrors
      annotations:
        description: '{{ $value | humanizePercentage }} of reconciling operations
          failed for {{ $labels.controller }} controller in {{ $labels.namespace }}
          namespace.'
        summary: Errors while reconciling objects.
      expr: |
        (sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_errors_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) / (sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) > 0.1
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusOperatorStatusUpdateErrors
      annotations:
        description: '{{ $value | humanizePercentage }} of status update operations
          failed for {{ $labels.controller }} controller in {{ $labels.namespace }}
          namespace.'
        summary: Errors while updating objects status.
      expr: |
        (sum by (cluster,controller,namespace) (rate(prometheus_operator_status_update_errors_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) / (sum by (cluster,controller,namespace) (rate(prometheus_operator_status_update_operations_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]))) > 0.1
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusOperatorNodeLookupErrors
      annotations:
        description: Errors while reconciling Prometheus in {{ $labels.namespace }}
          Namespace.
        summary: Errors while reconciling Prometheus.
      expr: |
        rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0.1
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusOperatorNotReady
      annotations:
        description: Prometheus operator in {{ $labels.namespace }} namespace isn't
          ready to reconcile {{ $labels.controller }} resources.
        summary: Prometheus operator not ready
      expr: |
        min by (cluster,controller,namespace) (max_over_time(prometheus_operator_ready{job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) == 0)
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: PrometheusOperatorRejectedResources
      annotations:
        description: Prometheus operator in {{ $labels.namespace }} namespace rejected
          {{ printf "%0.0f" $value }} {{ $labels.controller }}/{{ $labels.resource
          }} resources.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusOperatorRejectedResources.md
        summary: Resources rejected by Prometheus operator
      expr: |
        min_over_time(prometheus_operator_managed_resources{state="rejected",job="prometheus-operator", namespace=~"openshift-monitoring|openshift-user-workload-monitoring"}[5m]) > 0
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: config-reloaders
    rules:
    - alert: ConfigReloaderSidecarErrors
      annotations:
        description: |-
          Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.
          As a result, configuration for service running in {{$labels.pod}} may be stale and cannot be updated anymore.
        summary: config-reloader sidecar has not had a successful reload for 10m
      expr: |
        max_over_time(reloader_last_reload_successful{namespace=~".+"}[5m]) == 0
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/prometheus.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: prometheus
spec:
  groups:
  - name: default-storage-classes.rules
    rules:
    - alert: MultipleDefaultStorageClasses
      annotations:
        description: |
          Cluster storage operator monitors all storage classes configured in the cluster
          and checks there is not more than one default StorageClass configured.
        message: StorageClass count check is failing (there should not be more than
          one default StorageClass)
        summary: More than one default StorageClass detected.
      expr: min_over_time(default_storage_class_count[5m]) > 1
      for: 10m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
  - name: storage-operations.rules
    rules:
    - alert: PodStartupStorageOperationsFailing
      annotations:
        description: |
          Failing storage operation "{{ $labels.operation_name }}" of volume plugin {{ $labels.volume_plugin }} was preventing Pods on node {{ $labels.node }}
          from starting for past 5 minutes.
          Please investigate Pods that are "ContainerCreating" on the node: "oc get pod --field-selector=spec.nodeName={{ $labels.node }} --all-namespaces | grep ContainerCreating".
          Events of the Pods should contain exact error message: "oc describe pod -n <pod namespace> <pod name>".
        summary: Pods can't start because {{ $labels.operation_name }} of volume plugin
          {{ $labels.volume_plugin }} is permanently failing on node {{ $labels.node
          }}.
      expr: |
        increase(storage_operation_duration_seconds_count{status != "success", operation_name =~"volume_attach|volume_mount"}[5m]) > 0
          and ignoring(status) (sum without(status)
            (increase(storage_operation_duration_seconds_count{status = "success", operation_name =~"volume_attach|volume_mount"}[5m])
              or increase(storage_operation_duration_seconds_count{status != "success", operation_name =~"volume_attach|volume_mount"}[5m]) * 0)
            ) == 0
      for: 5m
      labels:
        severity: info
        customer: "true"
        namespace: cluster-monitoring
  - name: storage-selinux.rules
    rules:
    - expr: sum(volume_manager_selinux_pod_context_mismatch_warnings_total) + sum(volume_manager_selinux_pod_context_mismatch_errors_total)
      record: cluster:volume_manager_selinux_pod_context_mismatch_total
    - expr: sum by(volume_plugin) (volume_manager_selinux_volume_context_mismatch_warnings_total{volume_plugin
        !~".*-e2e-.*"})
      record: cluster:volume_manager_selinux_volume_context_mismatch_warnings_total
    - expr: sum by(volume_plugin) (volume_manager_selinux_volume_context_mismatch_errors_total{volume_plugin
        !~".*-e2e-.*"})
      record: cluster:volume_manager_selinux_volume_context_mismatch_errors_total
    - expr: sum by(volume_plugin) (volume_manager_selinux_volumes_admitted_total{volume_plugin
        !~".*-e2e-.*"})
      record: cluster:volume_manager_selinux_volumes_admitted_total
  - name: kubernetes-storage
    rules:
    - alert: KubePersistentVolumeFillingUp
      annotations:
        description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
          }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster
          {{ . }} {{- end }} is only {{ $value | humanizePercentage }} free.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md
        summary: PersistentVolume is filling up.
      expr: |
        (
          kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
            /
          kubelet_volume_stats_capacity_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
        ) < 0.03
        and
        kubelet_volume_stats_used_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
      for: 1m
      labels:
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubePersistentVolumeFillingUp
      annotations:
        description: Based on recent sampling, the PersistentVolume claimed by {{
          $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{
          with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to fill
          up within four days. Currently {{ $value | humanizePercentage }} is available.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md
        summary: PersistentVolume is filling up.
      expr: |
        (
          kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
            /
          kubelet_volume_stats_capacity_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
        ) < 0.15
        and
        kubelet_volume_stats_used_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
        and
        predict_linear(kubelet_volume_stats_available_bytes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubePersistentVolumeInodesFillingUp
      annotations:
        description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
          }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster
          {{ . }} {{- end }} only has {{ $value | humanizePercentage }} free inodes.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md
        summary: PersistentVolumeInodes are filling up.
      expr: |
        (
          kubelet_volume_stats_inodes_free{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
            /
          kubelet_volume_stats_inodes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
        ) < 0.03
        and
        kubelet_volume_stats_inodes_used{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
      for: 1m
      labels:
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubePersistentVolumeInodesFillingUp
      annotations:
        description: Based on recent sampling, the PersistentVolume claimed by {{
          $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{
          with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to run
          out of inodes within four days. Currently {{ $value | humanizePercentage
          }} of its inodes are free.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md
        summary: PersistentVolumeInodes are filling up.
      expr: |
        (
          kubelet_volume_stats_inodes_free{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
            /
          kubelet_volume_stats_inodes{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}
        ) < 0.15
        and
        kubelet_volume_stats_inodes_used{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"} > 0
        and
        predict_linear(kubelet_volume_stats_inodes_free{namespace=~"(openshift-.*|kube-.*|default)",job="kubelet", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_access_mode{namespace=~"(openshift-.*|kube-.*|default)", access_mode="ReadOnlyMany"} == 1
        unless on(cluster, namespace, persistentvolumeclaim)
        kube_persistentvolumeclaim_labels{namespace=~"(openshift-.*|kube-.*|default)",label_alerts_k8s_io_kube_persistent_volume_filling_up="disabled"} == 1
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: KubePersistentVolumeErrors
      annotations:
        description: The persistent volume {{ $labels.persistentvolume }} {{ with
          $labels.cluster -}} on Cluster {{ . }} {{- end }} has status {{ $labels.phase
          }}.
        summary: PersistentVolume is having issues with provisioning.
      expr: |
        kube_persistentvolume_status_phase{phase=~"Failed|Pending",namespace=~"(openshift-.*|kube-.*|default)",job="kube-state-metrics"} > 0
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/samples-operator-alerts.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: samples-operator-alerts
spec:
  groups:
  - name: SamplesOperator
    rules:
    - alert: SamplesRetriesMissingOnImagestreamImportFailing
      annotations:
        description: |
          Samples operator is detecting problems with imagestream image imports, and the periodic retries of those
          imports are not occurring.  Contact support.  You can look at the "openshift-samples" ClusterOperator object
          for details. Most likely there are issues with the external image registry hosting the images that need to
          be investigated.  The list of ImageStreams that have failing imports are:
          {{ range query "openshift_samples_failed_imagestream_import_info > 0" }}
            {{ .Labels.name }}
          {{ end }}
          However, the list of ImageStreams for which samples operator is retrying imports is:
          retrying imports:
          {{ range query "openshift_samples_retry_imagestream_import_total > 0" }}
             {{ .Labels.imagestreamname }}
          {{ end }}
        summary: Samples operator is having problems with imagestream imports and
          its retries.
      expr: sum(openshift_samples_failed_imagestream_import_info) > sum(openshift_samples_retry_imagestream_import_total)
        - sum(openshift_samples_retry_imagestream_import_total offset 30m)
      for: 2h
      labels:
        #namespace: openshift-cluster-samples-operator
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: SamplesImagestreamImportFailing
      annotations:
        description: |
          Samples operator is detecting problems with imagestream image imports.  You can look at the "openshift-samples"
          ClusterOperator object for details. Most likely there are issues with the external image registry hosting
          the images that needs to be investigated.  Or you can consider marking samples operator Removed if you do not
          care about having sample imagestreams available.  The list of ImageStreams for which samples operator is
          retrying imports:
          {{ range query "openshift_samples_retry_imagestream_import_total > 0" }}
             {{ .Labels.imagestreamname }}
          {{ end }}
        summary: Samples operator is detecting problems with imagestream image imports
      expr: sum(openshift_samples_retry_imagestream_import_total) - sum(openshift_samples_retry_imagestream_import_total
        offset 30m) > sum(openshift_samples_failed_imagestream_import_info)
      for: 2h
      labels:
        #namespace: openshift-cluster-samples-operator
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: SamplesDegraded
      annotations:
        description: |
          Samples could not be deployed and the operator is degraded. Review the "openshift-samples" ClusterOperator object for further details.
        summary: Samples operator is degraded.
      expr: openshift_samples_degraded_info == 1
      for: 2h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: SamplesInvalidConfig
      annotations:
        description: |
          Samples operator has been given an invalid configuration.
        summary: Samples operator Invalid configuration
      expr: openshift_samples_invalidconfig_info == 1
      for: 2h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: SamplesMissingSecret
      annotations:
        description: |
          Samples operator cannot find the samples pull secret in the openshift namespace.
        summary: Samples operator is not able to find secret
      expr: openshift_samples_invalidsecret_info{reason="missing_secret"} == 1
      for: 2h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: SamplesMissingTBRCredential
      annotations:
        description: |
          The samples operator cannot find credentials for 'registry.redhat.io'. Many of the sample ImageStreams will fail to import unless the 'samplesRegistry' in the operator configuration is changed.
        summary: Samples operator is not able to find the credentials for registry
      expr: openshift_samples_invalidsecret_info{reason="missing_tbr_credential"}
        == 1
      for: 2h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: SamplesTBRInaccessibleOnBoot
      annotations:
        description: |
          One of two situations has occurred.  Either
          samples operator could not access 'registry.redhat.io' during its initial installation and it bootstrapped as removed.
          If this is expected, and stems from installing in a restricted network environment, please note that if you
          plan on mirroring images associated with sample imagestreams into a registry available in your restricted
          network environment, and subsequently moving samples operator back to 'Managed' state, a list of the images
          associated with each image stream tag from the samples catalog is
          provided in the 'imagestreamtag-to-image' config map in the 'openshift-cluster-samples-operator' namespace to
          assist the mirroring process.
          Or, the use of allowed registries or blocked registries with global imagestream configuration will not allow
          samples operator to create imagestreams using the default image registry 'registry.redhat.io'.
        summary: Samples operator is not able to access the registry on boot
      expr: openshift_samples_tbr_inaccessible_info == 1
      for: 2d
      labels:
        severity: info
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/telemetry.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: telemetry
spec:
  groups:
  - name: telemeter.rules
    rules:
    - expr: max(federate_samples - federate_filtered_samples)
      record: cluster:telemetry_selected_series:count
    - alert: TelemeterClientFailures
      annotations:
        description: |-
          The telemeter client in namespace {{ $labels.namespace }} fails {{ $value | humanize }} of the requests to the telemeter service.
          Check the logs of the telemeter-client pod with the following command:
          oc logs -n openshift-monitoring deployment.apps/telemeter-client -c telemeter-client
          If the telemeter client fails to authenticate with the telemeter service, make sure that the global pull secret is up to date, see https://docs.openshift.com/container-platform/latest/openshift_images/managing_images/using-image-pull-secrets.html#images-update-global-pull-secret_using-image-pull-secrets for more details.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/TelemeterClientFailures.md
        summary: Telemeter client fails to send metrics
      expr: |
        sum by (namespace) (
          rate(federate_requests_failed_total{job="telemeter-client"}[15m])
        ) /
        sum by (namespace) (
          rate(federate_requests_total{job="telemeter-client"}[15m])
        ) > 0.2
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/thanos-querier.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: thanos-querier
spec:
  groups:
  - name: thanos-query
    rules:
    - alert: ThanosQueryHttpRequestQueryErrorRateHigh
      annotations:
        description: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing
          to handle {{$value | humanize}}% of "query" requests.
        summary: Thanos Query is failing to handle requests.
      expr: |
        (
          sum by (namespace, job) (rate(http_requests_total{code=~"5..", job="thanos-querier", handler="query"}[5m]))
        /
          sum by (namespace, job) (rate(http_requests_total{job="thanos-querier", handler="query"}[5m]))
        ) * 100 > 5
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosQueryHttpRequestQueryRangeErrorRateHigh
      annotations:
        description: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing
          to handle {{$value | humanize}}% of "query_range" requests.
        summary: Thanos Query is failing to handle requests.
      expr: |
        (
          sum by (namespace, job) (rate(http_requests_total{code=~"5..", job="thanos-querier", handler="query_range"}[5m]))
        /
          sum by (namespace, job) (rate(http_requests_total{job="thanos-querier", handler="query_range"}[5m]))
        ) * 100 > 5
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosQueryGrpcServerErrorRate
      annotations:
        description: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing
          to handle {{$value | humanize}}% of requests.
        summary: Thanos Query is failing to handle requests.
      expr: |
        (
          sum by (namespace, job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job="thanos-querier"}[5m]))
        /
          sum by (namespace, job) (rate(grpc_server_started_total{job="thanos-querier"}[5m]))
        * 100 > 5
        )
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosQueryGrpcClientErrorRate
      annotations:
        description: Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing
          to send {{$value | humanize}}% of requests.
        summary: Thanos Query is failing to send requests.
      expr: |
        (
          sum by (namespace, job) (rate(grpc_client_handled_total{grpc_code!="OK", job="thanos-querier"}[5m]))
        /
          sum by (namespace, job) (rate(grpc_client_started_total{job="thanos-querier"}[5m]))
        ) * 100 > 5
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosQueryHighDNSFailures
      annotations:
        description: Thanos Query {{$labels.job}} in {{$labels.namespace}} have {{$value
          | humanize}}% of failing DNS queries for store endpoints.
        summary: Thanos Query is having high number of DNS failures.
      expr: |
        (
          sum by (namespace, job) (rate(thanos_query_store_apis_dns_failures_total{job="thanos-querier"}[5m]))
        /
          sum by (namespace, job) (rate(thanos_query_store_apis_dns_lookups_total{job="thanos-querier"}[5m]))
        ) * 100 > 1
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosQueryOverload
      annotations:
        description: Thanos Query {{$labels.job}} in {{$labels.namespace}} has been
          overloaded for more than 15 minutes. This may be a symptom of excessive
          simultaneous complex requests, low performance of the Prometheus API, or
          failures within these components. Assess the health of the Thanos query
          instances, the connected Prometheus instances, look for potential senders
          of these requests and then contact support.
        summary: Thanos query reaches its maximum capacity serving concurrent requests.
      expr: |
        (
          max_over_time(thanos_query_concurrent_gate_queries_max[5m]) - avg_over_time(thanos_query_concurrent_gate_queries_in_flight[5m]) < 1
        )
      for: 1h
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
---
# Source: cluster-monitoring/templates/prometheusrules/thanos-ruler.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    role: custom-alerts
  namespace: cluster-monitoring
  name: thanos-ruler
spec:
  groups:
  - name: thanos-rule
    rules:
    - alert: ThanosRuleQueueIsDroppingAlerts
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is
          failing to queue alerts.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/ThanosRuleQueueIsDroppingAlerts.md
        summary: Thanos Rule is failing to queue alerts.
      expr: |
        sum by (namespace, job, instance) (rate(thanos_alert_queue_alerts_dropped_total{job="thanos-ruler"}[5m])) > 0
      for: 5m
      labels:
        severity: critical
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosRuleSenderIsFailingAlerts
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is
          failing to send alerts to alertmanager.
        summary: Thanos Rule is failing to send alerts to alertmanager.
      expr: |
        sum by (namespace, job, instance) (rate(thanos_alert_sender_alerts_dropped_total{job="thanos-ruler"}[5m])) > 0
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosRuleHighRuleEvaluationFailures
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is
          failing to evaluate rules.
        summary: Thanos Rule is failing to evaluate rules.
      expr: |
        (
          sum by (namespace, job, instance) (rate(prometheus_rule_evaluation_failures_total{job="thanos-ruler"}[5m]))
        /
          sum by (namespace, job, instance) (rate(prometheus_rule_evaluations_total{job="thanos-ruler"}[5m]))
        * 100 > 5
        )
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosRuleHighRuleEvaluationWarnings
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has
          high number of evaluation warnings.
        summary: Thanos Rule has high number of evaluation warnings.
      expr: |
        sum by (namespace, job, instance) (rate(thanos_rule_evaluation_with_warnings_total{job="thanos-ruler"}[5m])) > 0
      for: 15m
      labels:
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosRuleRuleEvaluationLatencyHigh
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has
          higher evaluation latency than interval for {{$labels.rule_group}}.
        runbook_url: https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/ThanosRuleRuleEvaluationLatencyHigh.md
        summary: Thanos Rule has high rule evaluation latency.
      expr: |
        (
          sum by (namespace, job, instance, rule_group) (prometheus_rule_group_last_duration_seconds{job="thanos-ruler"})
        >
          sum by (namespace, job, instance, rule_group) (prometheus_rule_group_interval_seconds{job="thanos-ruler"})
        )
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosRuleGrpcErrorRate
      annotations:
        description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} is failing
          to handle {{$value | humanize}}% of requests.
        summary: Thanos Rule is failing to handle grpc requests.
      expr: |
        (
          sum by (namespace, job, instance) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job="thanos-ruler"}[5m]))
        /
          sum by (namespace, job, instance) (rate(grpc_server_started_total{job="thanos-ruler"}[5m]))
        * 100 > 5
        )
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosRuleConfigReloadFailure
      annotations:
        description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has not
          been able to reload its configuration.
        summary: Thanos Rule has not been able to reload configuration.
      expr: avg by (namespace, job, instance) (thanos_rule_config_last_reload_successful{job="thanos-ruler"})
        != 1
      for: 5m
      labels:
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosRuleQueryHighDNSFailures
      annotations:
        description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has {{$value
          | humanize}}% of failing DNS queries for query endpoints.
        summary: Thanos Rule is having high number of DNS failures.
      expr: |
        (
          sum by (namespace, job, instance) (rate(thanos_rule_query_apis_dns_failures_total{job="thanos-ruler"}[5m]))
        /
          sum by (namespace, job, instance) (rate(thanos_rule_query_apis_dns_lookups_total{job="thanos-ruler"}[5m]))
        * 100 > 1
        )
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosRuleAlertmanagerHighDNSFailures
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has
          {{$value | humanize}}% of failing DNS queries for Alertmanager endpoints.
        summary: Thanos Rule is having high number of DNS failures.
      expr: |
        (
          sum by (namespace, job, instance) (rate(thanos_rule_alertmanagers_dns_failures_total{job="thanos-ruler"}[5m]))
        /
          sum by (namespace, job, instance) (rate(thanos_rule_alertmanagers_dns_lookups_total{job="thanos-ruler"}[5m]))
        * 100 > 1
        )
      for: 15m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosRuleNoEvaluationFor10Intervals
      annotations:
        description: Thanos Rule {{$labels.job}} in {{$labels.namespace}} has rule
          groups that did not evaluate for at least 10x of their expected interval.
        summary: Thanos Rule has rule groups that did not evaluate for 10 intervals.
      expr: |
        time() -  max by (namespace, job, instance, group) (prometheus_rule_group_last_evaluation_timestamp_seconds{job="thanos-ruler"})
        >
        10 * max by (namespace, job, instance, group) (prometheus_rule_group_interval_seconds{job="thanos-ruler"})
      for: 5m
      labels:
        severity: info
        customer: "true"
        namespace: cluster-monitoring
    - alert: ThanosNoRuleEvaluations
      annotations:
        description: Thanos Rule {{$labels.instance}} in {{$labels.namespace}} did
          not perform any rule evaluations in the past 10 minutes.
        summary: Thanos Rule did not perform any rule evaluations.
      expr: |
        sum by (namespace, job, instance) (rate(prometheus_rule_evaluations_total{job="thanos-ruler"}[5m])) <= 0
          and
        sum by (namespace, job, instance) (thanos_rule_loaded_rules{job="thanos-ruler"}) > 0
      for: 5m
      labels:
        severity: warning
        customer: "true"
        namespace: cluster-monitoring
